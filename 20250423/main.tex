\documentclass[9pt]{beamer}
\input{commands}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage[sort]{natbib}
\usepackage{adjustbox}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\AtBeginDocument{\RenewCommandCopy\qty\SI}
\pgfplotsset{compat=newest}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}
\setbeamerfont{title}{size=\huge}


\let\ab\allowbreak

\title{Mastering Memory Tasks with World Models}
\date{April 23, 2025}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\begin{frame}
    \frametitle{RNNs vs. Transformers for Long Sequences}
    \begin{itemize}
        \item \textbf{World models} in RL capture environment dynamics
            \begin{itemize}
                \item Enable agents to perceive, simulate, and plan
                \item Learn from past experiences to predict future states
            \end{itemize}
        \item \textbf{Key challenges}:
            \begin{itemize}
                \item Credit assignment problem
                \item Memorizing and recalling past experiences
                \item Learning long-range dependencies
            \end{itemize}
        \item \textbf{RNNs} (common in MBRL):
            \begin{itemize}
                \item Handle sequential data
                \item Limited by vanishing gradients
            \end{itemize}
        \item \textbf{Transformers}:
            \begin{itemize}
                \item Successful in language modeling
                \item Quadratic complexity in sequence length
                \item Unstable during training on long sequences
            \end{itemize}
        \item \textbf{State-Space Models (SSMs)}:
            \begin{itemize}
                \item Effectively capture dependencies in long sequences
                \item S4 model redefines long-range sequence modeling
                \item Can handle dependencies up to 16K in length
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Introducing Recall to Imagine (R2I) \citep{samsami2024mastering}}
    \begin{itemize}
        \item \textbf{R2I}: First MBRL approach using a variant of S4
            \begin{itemize}
                \item Built upon DreamerV3
                \item Empowers agents with long-term memory
                \item Computationally efficient (up to 9x faster)
            \end{itemize}
        \item \textbf{Key contributions}:
            \begin{itemize}
                \item Memory-enhanced MBRL agent using modified S4
                \item State-of-the-art performance in memory domains:
                    \begin{itemize}
                        \item POPGym
                        \item Behavior Suite (BSuite)
                        \item Memory Maze (outperforms humans)
                    \end{itemize}
                \item Maintains strong performance in standard benchmarks:
                    \begin{itemize}
                        \item Atari
                        \item DeepMind Control Suite (DMC)
                    \end{itemize}
                \item Comprehensive ablation studies validate design decisions
            \end{itemize}
        \item R2I offers a general solution for tasks requiring long-term memory or credit assignment
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{State-Space Models: Continuous View}
    \begin{itemize}
        \item SSMs describe hidden state evolution over time:
        \begin{align*}
        x'(t) &= \mathbf{A}\,x(t) \;+\; \mathbf{B}\,u(t),\\
        y(t) &= \mathbf{C}\,x(t) \;+\; \mathbf{D}\,u(t).
        \end{align*}
        \item Key components:
        \begin{itemize}
            \item $x(t)\in\mathbb{C}^N$: hidden "memory" of size $N$
            \item $\mathbf{A}$: controls state evolution
            \item $\mathbf{B}$: injects new input $u(t)$
            \item $\mathbf{C}$, $\mathbf{D}$: read out state to form output $y$
        \end{itemize}
        \item Continuous-time model must be discretized for ML tasks
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Discretization and S4 Architecture}
    \begin{itemize}
        \item Discretizing with time step $\Delta$ gives recurrence:
        \begin{align*}
        x_n &= \Bar{\mathbf{A}}\,x_{n-1} \;+\; \Bar{\mathbf{B}}\,u_n,\\
        y_n &= \Bar{\mathbf{C}}\,x_n \;+\; \Bar{\mathbf{D}}\,u_n,
        \end{align*}
        \item \textbf{S4 innovations}:
        \begin{itemize}
            \item Uses diagonal + low-rank (DPLR) structure for $\mathbf{A}$
            \item \textbf{HiPPO initialization}: 
            \begin{itemize}
                \item Decomposes $u(t)$ into infinite basis functions
                \item Enables capturing long-range dependencies
            \end{itemize}
            \item Instead of stepping through time one step at a time (as an RNN would), S4 shows that the entire mapping  
            \[
              (u_{1:T},\,x_0)\;\longmapsto\;(y_{1:T},\,x_T)
            \]
            can be performed as a single 1D convolution over the sequence.
            \item \textbf{Parallel scan}: Efficient computation of entire sequence
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{S4 Advantages for RL World Models}
    \begin{itemize}
        \item \textbf{S4 as a fusion}:
        \begin{itemize}
            \item Combines CNNs, RNNs, and classical SSMs
            \item Outperforms Transformers in inference speed and memory consumption
            \item Recurrent inference mode provides efficiency
        \end{itemize}
        \item \textbf{Benefits for RL world models}:
        \begin{itemize}
            \item \textbf{Long-term credit assignment} becomes feasible
            \item \textbf{Training speed} increases significantly
            \item \textbf{Stability} improves on long rollouts
            \item Avoids vanishing gradients (RNNs)
            \item Prevents instabilities at long sequence lengths (Transformers)
        \end{itemize}
        \item Impressive empirical results on benchmarks involving long dependencies
        \item Recent refinements focus on understanding and improving S4
        \item Ideal backbone for world models in MBRL
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Non-recurrent Representation Model in R2I}
    \begin{itemize}
        \item \textbf{Original coupled (recurrent) setup}:
        \begin{itemize}
            \item Representation model: $q_\theta(z_t \mid h_t, o_t)$
            \item Sequence model: $(h_t, x_t) = f_\theta(h_{t-1}, z_{t-1}, a_{t-1})$
            \item Circular dependency: $z_t$ depends on $h_t$, which depends on $z_{t-1}$
            \item Forces strict time-step ordering, preventing parallelization
        \end{itemize}
        \item \textbf{Breaking the cycle with non-recurrent representation}:
        \begin{itemize}
            \item Simplify to $q_\theta(z_t \mid o_t)$ by dropping $h_t$ dependency
            \item Each posterior $z_t$ inferred only from observation $o_t$
            \item All $z_1, z_2, \ldots, z_T$ can be computed in parallel
        \end{itemize}
        \item \textbf{Parallel sequence modeling with SSMs}:
        \begin{itemize}
            \item Feed full batch of latents and actions: $h_{1:T}, x_{1:T} = f_\theta((a_{1:T}, z_{1:T}), x_0)$
            \item S4-style SSM enables convolutional/parallel-scan implementation
            \item Entire sequence processed in one parallel operation
        \end{itemize}
        \item \textbf{Benefits}: Full parallelism, no performance loss, much faster world model updates
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{R2I Architecture: S3M + Prediction Heads}
    \begin{itemize}
        \item \textbf{S3M} (Sequence-State-Space Model) is the core world model:
        \begin{itemize}
            \item \textbf{Representation model}: $q_\theta(z_t \mid o_t)$ infers latent state
            \item \textbf{Dynamics model}: Implicit in training losses
            \item \textbf{Sequence model}: Updates deterministic belief state $h_t$
        \end{itemize}
        \item \textbf{Three prediction heads} built on top of S3M:
        \begin{itemize}
            \item \textbf{Observation predictor}: $p_\theta(\hat{o}_t \mid z_t, h_t)$
            \item \textbf{Reward predictor}: $p_\theta(\hat{r}_t \mid z_t, h_t)$
            \item \textbf{Continuation predictor}: $p_\theta(\hat{c}_t \mid z_t, h_t)$
        \end{itemize}
        \item At each time step $t$, S3M processes:
        \[
            (h_t, x_t) = f_\theta((a_{t-1}, z_{t-1}), x_{t-1})
        \]
        where $x_t$ represents all internal SSM layer states and $h_t$ is the final output
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How the Sequence Model Works}
    \begin{itemize}
        \item \textbf{$f_\theta$ architecture} - stack of SSM layers:
        \begin{itemize}
            \item Each layer applies discrete-time SSM update:
            \[
                x_n = \bar{\mathbf{A}}\,x_{n-1} + \bar{\mathbf{B}}\,u_n, \quad
                y_n = \bar{\mathbf{C}}\,x_n + \bar{\mathbf{D}}\,u_n
            \]
            where $u_n$ is the concatenation $(a_{n-1}, z_{n-1})$
            \item Processing pipeline for each layer:
            \[
                \text{SSM} \rightarrow \text{GeLU} \rightarrow \text{GLU} \rightarrow \text{LayerNorm}
            \]
        \end{itemize}
        \item \textbf{Key advantages}:
        \begin{itemize}
            \item Modality-agnostic sequence core with specialized I/O
            \item Parallel processing of long sequences: $\mathcal{O}(L \log L)$ complexity
            \item Stable training with rich temporal dynamics
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{R2I: Recurrent World Model Architecture}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/r2i.png}
        \caption{Graphical representation of R2I. \textbf{(Left)} The world model encodes past experiences, transforming observations and actions into compact latent states. Reconstructing the trajectories serves as a learning signal for shaping these latent states. \textbf{(Right)} The policy learns from trajectories based on latent states imagined by the world model. The representation corresponds to the full state policy, and we have omitted the critic for the sake of simplifying the illustration.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Training Details}
    \begin{itemize}
        \item \textbf{Optimization objective}:
        \begin{align*}
            \mathcal{L}(\theta) = \mathop{\mathbb{E}}_{z_{1:T}\sim q_\theta} \sum_{t=1}^{T}&\mathcal{L}^{\text{pred}}(\theta,  h_t, o_t, r_t, c_t, z_t) 
            + \mathcal{L}^{\text{rep}}(\theta, h_t, o_t) + \mathcal{L}^{\text{dyn}}(\theta, h_t, o_t)
        \end{align*}
        
        \item \textbf{Loss components}:
        \begin{itemize}
            \item \textbf{Prediction loss}: 
            \begin{align*}
                \mathcal{L}^{\text{pred}} = -\beta_{\text{pred}}(\ln p_\theta(o_t\mid z_t, h_t) + \ln p_\theta(r_t\mid z_t, h_t)
                + \ln p_\theta(c_t\mid z_t, h_t))
            \end{align*}
            
            \item \textbf{Dynamics loss}: 
            \begin{align*}
                \mathcal{L}^{\text{dyn}} = \beta_{\text{dyn}}\max(1, \text{KL}[\texttt{sg}(q_\theta(z_t\mid o_t))\, \| \,p(z_t \mid h_t)])
            \end{align*}
            
            \item \textbf{Representation loss}: 
            \begin{align*}
                \mathcal{L}^{\text{rep}} = \beta_{\text{rep}}\,\max(1, \text{KL}[q_\theta(z_t\mid o_t)\,\| \texttt{sg}(p(z_t \mid h_t))])
            \end{align*}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SSMs Computational Modeling}
    \begin{itemize}
        \item \textbf{Parallelizability options}: 
        \begin{itemize}
            \item Convolution (Gu et al., 2021) vs. parallel scan (Smith et al., 2023)
            \item Parallel scan chosen for several key advantages
        \end{itemize}
        
        \item \textbf{Key reasons for choosing parallel scan}:
        \begin{itemize}
            \item Essential to pass hidden states $x_t$ to policy in memory environments
            \item Avoids burn-in steps needed with convolution mode
            \item Enables scaling of sequence length across distributed devices
            \item Facilitates resetting of hidden states between episodes
        \end{itemize}
    \end{itemize}
    
    \begin{table}
        \centering
        \begin{adjustbox}{width=0.9\textwidth}
        \begin{tabular}{r|ccccc}
         Method & Training & \makecell[c]{Inference \\ step} & \makecell[c]{Imagination \\ step} & Parallel & \makecell{State \\ Reset}\\
        \midrule
        Attention & $\mathcal{O}(L^2)$ & $\mathcal{O}(L^2)$ & $\mathcal{O}((L+H)^2)$ & $\checkmark$ & $\checkmark$ \\
        RNN & $\mathcal{O}(L)$ & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ & $\times$ & $\checkmark$\\
        SSM (Conv) & $\mathcal{O}(L)$ & $\mathcal{O}(1)$ & $\mathcal{O}(L)$ & $\checkmark$ & $\times$ \\
        \midrule
        SSM (Par.Scan) & $\mathcal{O}(L)$ & $\mathcal{O}(1)$ & $\mathcal{O}(1)$ & $\checkmark$ & $\checkmark$ \\
        \end{tabular}
        \end{adjustbox}
        \caption{Asymptotic runtimes of different architectures. $L$ is sequence length and $H$ is imagination horizon. SSMs with parallel scan achieve optimal complexity by combining recurrence with parallel computation.}
        \label{tab:asymp}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{SSMs Computational Modeling (continued)}
    \begin{itemize}
        \item \textbf{Computational complexity comparison}:
        \begin{itemize}
            \item Attention models: Quadratic complexity $\mathcal{O}(L^2)$ during training
            \item RNNs: Linear complexity but sequential processing (not parallelizable)
            \item SSM with convolution: Requires burn-in steps, leading to $\mathcal{O}(L)$ imagination complexity
            \item SSM with parallel scan: Combines best properties - parallel processing with $\mathcal{O}(1)$ imagination steps
        \end{itemize}
        
        \item \textbf{Hidden state reset capability}:
        \begin{itemize}
            \item Critical when sampling sequences with multiple episodes from buffer
            \item Hidden states must reset from terminal states to initial states in new episodes
            \item Improves early training performance when episodes are short
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model States and Policy Conditioning}
    \begin{itemize}
        \item \textbf{Two distinct states in R2I's world model}:
        \begin{itemize}
            \item \textbf{Deterministic state} $h_t$: Output of final SSM layer at time $t$
            \begin{itemize}
                \item Analogous to what DreamerV3 feeds to reconstruction heads and GRU
            \end{itemize}
            \item \textbf{SSM hidden state} $x_t$: Packs every layer's internal memory at time $t$
            \begin{itemize}
                \item Only $x_t$ is carried forward to next SSM update; $h_t$ is not
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Policy conditioning variants}:
        \begin{itemize}
            \item Output-state policy: $\pi(a_t\mid z_t,\,h_t)$
            \item Hidden-state policy: $\pi(a_t\mid z_t,\,x_t)$
            \item Full-state policy: $\pi(a_t\mid z_t,\,h_t,\,x_t)$
        \end{itemize}
        
        \item \textbf{Key empirical findings}:
        \begin{itemize}
            \item In memory-intensive tasks, using $(z_t,h_t)$ breaks policy learning
            \item Using $(z_t,x_t)$ restores stable, high-performance learning
            \item Full-state policy $(z_t,h_t,x_t)$ fails due to non-stationarity in joint distribution
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical Recipe for Policy Conditioning}
    \begin{itemize}
        \item \textbf{Task-dependent policy conditioning}:
        \begin{itemize}
            \item \textbf{Vector or non-memory tasks}: Use output-state policy $(z_t,h_t)$
            \item \textbf{Memory-heavy tasks}: Use hidden-state policy $(z_t,x_t)$
            \item Full-state policy $(z_t,h_t,x_t)$ not recommended due to non-stationarity
        \end{itemize}
        
        \item \textbf{Actor-Critic training}:
        \begin{itemize}
            \item Adopts same scheme from DreamerV3
            \item Imagined rollouts generated by policy inside learned world model
            \item Actor maximizes expected imagined returns
            \item Critic (value network) predicts those returns
            \item Gradients flow through both policy and world model
        \end{itemize}
        
        \item \textbf{Key insight}: When separating SSM's internal memory $x_t$ from output $h_t$, policies trained on $x_t$ (plus $z_t$) are far more stable in long-horizon, memory-demanding tasks
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Computational Efficiency Analysis}
    \begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{figures/timing.pdf}
        \caption{Computational time taken by DreamerV3 and R2I (lower is preferred)}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Quantifying Memory of R2I}
    \begin{itemize}
        \item \textbf{Memory stress tests} using BSuite tasks:
        \begin{itemize}
            \item \textbf{Memory Length}: Remember initial cue throughout episode
            \begin{itemize}
                \item First observation determines correct final action
                \item Challenge: Maintain memory across variable episode lengths
            \end{itemize}
            \item \textbf{Discounting Chain}: Link early action to delayed reward
            \begin{itemize}
                \item Initial action choice determines reward after fixed delay
                \item Challenge: Credit assignment across time
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Key result}: R2I achieves significantly higher success rates across wider range of episode lengths and reward delays.
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/bsuite.pdf}
        \caption{Success rate comparison between R2I and DreamerV3 on BSuite tasks}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Quantifying Memory of R2I (cont.)}

    \begin{itemize}
        \item \textbf{POPGym benchmark study}:
        \begin{itemize}
            \item Collection of RL environments for POMDP challenges
            \item Focus on navigation, noise robustness, and memory
        \end{itemize}
        
        \item \textbf{Selected memory-intensive environments}:
        \begin{itemize}
            \item \textbf{RepeatPrevious}: Recall and reproduce past actions
            \item \textbf{Autoencode}: Memorize and reconstruct observations
            \item \textbf{Concentration}: Track multiple objects simultaneously
        \end{itemize}
        
        \item \textbf{Difficulty levels} (Easy, Medium, Hard):
        
        \item \textbf{Key result}: R2I establishes new SOTA performance in these memory-intensive tasks
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/popgym_memory1_fix.pdf}
        \caption{Performance comparison of R2I against baseline methods on POPGym memory-intensive tasks across different difficulty levels}
    \end{figure}

    Combined with BSuite results, demonstrates R2I significantly pushes memory limits

\end{frame}

\begin{frame}
    \frametitle{Evaluating Long-term Memory In Complex 3D Tasks}
    
    \begin{itemize}
        \item \textbf{Memory Maze} (Pasukonis et al., 2022):
        \begin{itemize}
            \item Randomized 3D mazes with multiple objects to navigate to
            \item Requires agent to remember object locations, maze layout, and position
            \item Episodes extend up to 4K environment steps
            \item Ideal agent with long-term memory only needs to explore each maze once
            \item Existing memory-augmented RL algorithms significantly underperform humans
        \end{itemize}
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/mmaze_v3.pdf}
        \caption{Scores in Memory Maze after 400M environment steps. R2I outperforms baselines across difficulty levels, becoming the domain's new SOTA. Due to its enhanced computational efficiency, R2I was trained during a fewer number of days compared to Dreamer}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Generality of R2I in non-memory domains}
    
    \begin{itemize}
        \item \textbf{Evaluating R2I on standard benchmarks}:
        \begin{itemize}
            \item Tested on Atari and DMC - widely used non-memory RL benchmarks
            \item Essential to verify R2I maintains general capabilities
        \end{itemize}
        
        \item \textbf{Key finding}: R2I performs similarly to DreamerV3
        
        \item \textbf{Conclusion}: R2I enhances memory capabilities without sacrificing performance on standard tasks
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/non-mem.jpg}
        \caption{Performance comparison of R2I against baseline methods on standard non-memory-intensive reinforcement learning benchmarks}
    \end{figure}
\end{frame}




\end{document}