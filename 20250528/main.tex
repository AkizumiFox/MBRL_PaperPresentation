\documentclass[9pt]{beamer}
\input{commands}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage[sort]{natbib}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{cleveref}
\usepackage{adjustbox}
\usepackage{pifont}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\AtBeginDocument{\RenewCommandCopy\qty\SI}
\pgfplotsset{compat=newest}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}
\setbeamerfont{title}{size=\huge}

\newcommand{\rvx}{\mathbf{x}}
\newcommand{\rvz}{\mathbf{z}}
\newcommand{\E}{\mathbb{E}}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algnewcommand\algorithmiclinecomment[1]{\(\triangleright\) #1}
\algnewcommand\algorithmicplainlinecomment[1]{\phantom{\(\triangleright\)} #1}
\def\LineComment{\algorithmiclinecomment}
\def\PlainLineComment{\algorithmicplainlinecomment}

\AtBeginSection[]{
  \begin{frame}
    \vfill
    \centering
    \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
    \end{beamercolorbox}
    \vfill
  \end{frame}
}


\let\ab\allowbreak

\title{Pretrained VAEs on World Models}
\date{May 28, 2025}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\section{The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning}

\begin{frame}
    \frametitle{Research Questions}
    \begin{enumerate}
        \item Is MBRL more sample efficient when using PVRs (pre-trained visual representations) compared to learning from scratch?
        \item Can model-based agents generalize better to OOD settings with PVRs?
        \item How important are training properties (data diversity, architecture) of PVRs for downstream RL tasks?
        \item How does the quality of learned dynamics models differ between scratch-trained and PVR-based models?
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{\citep{schneider2025surprisingineffectivenesspretrainedvisual} Contributions}
    \begin{itemize}
        \item \textbf{Benchmarking PVRs for MBRL.} First comprehensive comparison of PVRs for MBRL, studying generalization to OOD settings
        \item \textbf{OOD Evaluation.} Investigated distribution shifts for both PVRs and MBRL agents, finding that scratch-trained agents often outperform PVR-based ones
        \item \textbf{PVR Properties.} Identified data diversity and network architecture as key factors for OOD generalization
        \item \textbf{Model Analysis.} Found that scratch-trained world models are generally more accurate with fewer reward prediction errors
    \end{itemize}
\end{frame}

\begin{frame}{PVR-based DreamerV3 and TD-MPC2 architectures}
  \begin{figure}[tb]
    \begin{center}
        \includegraphics[width=\linewidth]{figures/pvr_dreamer/pvr-dreamer.pdf}
    \end{center}
    \caption{\label{fig:pvr_dreamer} \textbf{Components of PVR-based DreamerV3 (left) and TD-MPC2 (right) architectures.} In DreamerV3, the output $x_t$ of the frozen pre-trained vision module $g_{\textrm{\ding{100}}}$ is given to the encoder $\mathrm{enc}(z_t \vert x_t)$ which maps its input to a discrete latent variable $\textrm{z}_t$. In TD-MPC2 a stack $x_{t-3:t}$ of the last $3$ PVR embeddings is given to the encoder $\mathrm{enc}(x_{t-3:t})$ which maps the inputs to fixed-dimensional simplices. The encoder of DreamerV3 additionally requires the recurrent state $h_t$ as input. The rest of both algorithms remains unchanged.}
    \vspace{-2ex}
\end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experimental Setup}
    \begin{itemize}
        \item \textbf{MBRL Algorithms:} DreamerV3 and TD-MPC2
        \begin{itemize}
            \item Freeze pre-trained vision backbone $g(o_t)$
            \item Learn single linear layer on frozen features
            \item Keep other components unchanged
        \end{itemize}
        
        \item \textbf{PVRs:} 8 models + custom autoencoders
        \begin{itemize}
            \item CLIP, R3M, Taskonomy, VIP, DINOv2, OpenCLIP, VC-1, R2D2
            \item Mix of self-supervised, contrastive, and autoencoding methods
            \item Custom autoencoders pre-trained on same images for ablation
        \end{itemize}
        
        \item \textbf{Environments:} 10 tasks across 3 domains
        \begin{itemize}
            \item DeepMind Control Suite (5 tasks)
            \item ManiSkill2 (4 tasks)
            \item Miniworld (1 task)
            \item All use 256×256 RGB observations
        \end{itemize}
        
        \item \textbf{Training:}
        \begin{itemize}
            \item DMC: 3M steps, ManiSkill2/Miniworld: 5M steps
            \item 12 short rollouts every 50k steps
            \item 200 full episodes for final evaluation
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Data Efficiency}
    \begin{itemize}
        \item \textbf{Research Question:} Are PVR-based MBRL agents more data efficient than scratch-trained ones?
        \item \textbf{Key Finding:} Scratch-trained agents often perform better or equal to PVR-based ones
        \item \textbf{Surprising Result:} Even autoencoders pre-trained on task-specific data don't help
        \item \textbf{Why?} Objective mismatch in MBRL makes it harder to adapt to existing representations
    \end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.95\linewidth]{figures/data_efficiency_aggregated/curves_domains}
		\caption{\label{fig:data_efficiency} \textbf{Normalized ID performance and data-efficiency comparison} on DMC, ManiSkill2 and Miniworld environments between the different representations. Each line represents the mean over all runs with a given representation, the shaded area represents the corresponding standard deviation. Solid lines represent DreamerV3 runs, whereas dashed lines indicate TD-MPC2 experiments. Especially in the DMC experiments, representations trained from scratch outperform all PVRs also in terms of data-efficiency. Curves of each environment individually can be found in Appendix.}
	\end{figure}
\end{frame}

\begin{frame}{Generalization to OOD Settings}
    \begin{itemize}
        \item \textbf{Research Question:} Do PVRs perform better than scratch-trained agents in OOD domains?
        
        \item \textbf{Key Findings:}
        \begin{itemize}
            \item Most PVRs don't generalize well across domains
            \item Only VC-1 shows some cross-domain capability
            \item Even task-specific autoencoders underperform
            \item Scratch-trained agents consistently outperform PVRs
        \end{itemize}
        
        \item \textbf{Surprising Result:} Despite being trained on diverse data, visual foundation models don't show better OOD generalization
        
        \item \textbf{Why?} The objective mismatch in MBRL makes it challenging to leverage pre-trained representations effectively
    \end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.95\linewidth]{figures/barchart_performance_ood/bars}
		\caption{\label{fig:bars_performance} \textbf{Average normalized performance on DMC, ManiSkill2 and Miniworld tasks in the OOD setting.} The baseline representation learned from scratch outperforms all PVRs, even in the OOD settings. Thin black lines denote the standard error.}
	\end{figure}
\end{frame}

\begin{frame}{Properties of PVRs for Generalization}
    \begin{itemize}
        \item \textbf{Research Question:} What properties make PVRs effective for OOD generalization?
        
        \item \textbf{Key Findings:}
        \begin{itemize}
            \item Language conditioning not necessary for good OOD performance
            \item Sequential data training somewhat beneficial (especially in ManiSkill2)
            \item Data diversity important for performance in both domains
            \item ViT architecture shows good performance but not the only factor
        \end{itemize}
        
        \item \textbf{Surprising Results:}
        \begin{itemize}
            \item Language conditioning, despite being popular, doesn't help control tasks
            \item Data diversity more important than network architecture
            \item ViT-based CLIPs perform similarly to ResNet-based ones
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.95\linewidth]{figures/properties_aggregated/scatter_aggregated_domains.pdf}
		\caption{\label{fig:properties} \textbf{IQM return of different categorizations.} Each marker shows interquartile-mean performance. ViT and diverse data representations excel in OOD settings. Sequential data helps in ManiSkill2 and Miniworld but not DMC. See Appendix for individual environment plots.}
	\end{figure}
\end{frame}
\begin{frame}{World Model Differences}
    \begin{itemize}
        \item \textbf{Research Question:} Why do PVR-based agents underperform despite having powerful vision backbones?
        
        \item \textbf{Key Findings:}
        \begin{itemize}
            \item Dynamics prediction error similar between PVR and scratch-trained agents
            \item Reward prediction error significantly higher for most PVRs
            \item Strong correlation between reward prediction accuracy and task performance
            \item Latent space organization differs: scratch-trained encoders better separate states by reward
        \end{itemize}
        
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item State prediction not the bottleneck - PVRs perform similarly to scratch
            \item Reward prediction is critical - only scratch-trained encoders learn reward-relevant features
            \item PVRs' frozen features lack reward-relevant information needed for planning
            \item This explains why PVR-based agents struggle compared to end-to-end learning
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}[t]
		\centering
		\begin{subfigure}[t]{0.9\linewidth}
			\includegraphics[width=\linewidth]{figures/dynamics_error/bar_cum_dynamics_error.pdf}
		\end{subfigure}
		\hfill
		\caption{\label{fig:dynamics_error} \textbf{Dynamics Prediction Errors} on \texttt{Pendulum Swingup} (200 trajectories). DreamerV3: KL divergence between prior/posterior distributions. TD-MPC2: MSE between predicted and encoded latent states. Error bars show standard error.}
	\end{figure}
	\begin{figure}[t]
		\centering
		\hfill
		\begin{subfigure}[t]{0.9\linewidth}
			\includegraphics[width=\linewidth]{figures/reward_error/bar_cum_reward_error.pdf}
		\end{subfigure}
		\caption{\label{fig:reward_error} \textbf{Reward Prediction Errors} on \texttt{Pendulum Swingup} (200 trajectories). Error is $\vert r_t - \hat{r}_t \vert$. Error bars show standard error.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figures/umap/umap_dreamerv3_reward_embed}
		\includegraphics[width=0.9\linewidth]{figures/umap/umap_tdmpc2_reward_latent_encoded}
		\caption{\label{fig:umaps}\textbf{UMAP projections of DreamerV3 (top) and TD-MPC2 (bottom) encodings} color-coded by reward. Points show states from \texttt{Pendulum Swingup}. Scratch-trained representations better separate high/low reward states than PVR embeddings.}
	\end{figure}
\end{frame}

\section{Efficient reinforcement learning through adaptively pretrained visual encoder}

\begin{frame}{APE \citep{zhang2025efficientreinforcementlearningadaptively} Pipeline}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.8\textwidth]{figs/pipeline1.pdf}
        \caption{APE pipeline for MBRL. Training consists of two phases: Adaptive Pretraining (blue) and Downstream Policy Learning (yellow). The first phase uses adaptive data augmentation on real-world images, dynamically adjusting augmentation probabilities. The second phase integrates the pretrained encoder as a perception module in the RL framework.}
        \label{fig1}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Two-Stage Architecture: Adaptive Pretraining (1/2)}
    \begin{itemize}
        \item \textbf{Backbone:} MoCo v2 contrastive framework
        \begin{itemize}
            \item Query encoder $f_q$ and momentum encoder $f_k$
            \item For each image, sample two augmentations to get embeddings:
            \begin{align*}
                q &= f_q(\text{view}_1) \\
                k^+ &= f_k(\text{view}_2)
            \end{align*}
            \item Negatives $\{k^-\}$ come from other images in batch/memory queue
            \item Contrastive loss (InfoNCE) between views:
            \begin{align*}
                \ell_q = -\log \frac{\exp(q \cdot k^+ / \tau)}{\exp(q \cdot k^+ / \tau) + \sum_{k^-}\exp(q\cdot k^- / \tau)}
            \end{align*}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Two-Stage Architecture: Adaptive Pretraining (2/2)}
    \begin{itemize}
        \item \textbf{Multiple Augmentation Compositions}
        \begin{itemize}
            \item Divide batch into $N$ sub-batches with distinct augmentations
            \item Weighted loss over compositions:
            \begin{align*}
                \mathcal{L}_z = \sum_{i=1}^N p_i\,\ell_q^{(i)}
            \end{align*}
            \item Closed-loop probability updates based on pretext accuracy:
            \begin{align*}
                p_i^{\,\text{new}} = \frac{\exp(\alpha\,(1 - \mathrm{Acc}_i))}{\sum_j \exp(\alpha\,(1 - \mathrm{Acc}_j))}
            \end{align*}
            where $\alpha=0.8$ for $N=7$ and $\alpha=1$ for $N=3$
            \item \textbf{Intuition:} Harder compositions ($\mathrm{Acc}_i \ll 1$) get more weight
        \end{itemize}
        
        \item \textbf{Key Outcome:}
        \begin{itemize}
            \item Encoder learns invariance to visual changes
            \item Mastery of diverse augmentation recipes
            \item Rich, general features for downstream RL
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Two-Stage Architecture: Downstream Policy Learning}
    It is exactly the structure of the DreamerV3 model.

    \begin{align*}
        \mathcal{L}_{rew}(\theta) = &-\log(p_{\theta}^{R}(\hat{r}_t \mid z_t,z_{t-1},a_{t-1}))\\
            \mathcal{L}_{con}(\theta)= &-\log(p_{\theta}^{C}(\hat{c}_t \mid z_t,z_{t-1},a_{t-1}))\\
            \mathcal{L}_{rec}(\theta)= &-\log(g_{\theta}(\hat{x}_t \mid z_t,z_{t-1},a_{t-1}))\\
            \mathcal{L}_{obs}(\theta) = & \beta_1 \max (1, \text{KL}[\text{sg}(f_{\theta}(z_t \mid z_{t-1},a_{t-1},o_t))\\
            & \parallel p_{\theta}^{D}(\hat{z}_t \mid z_{t-1},a_{t-1})])\\
            & + \beta_2 \max (1, \text{KL}[(f_{\theta}(z_t \mid z_{t-1},a_{t-1},o_t))\\
            & \parallel \text{sg}(p_{\theta}^{D}(\hat{z}_t \mid z_{t-1},a_{t-1}))])\\
    \end{align*}

    \begin{align*}
        \mathcal{L}(\theta)=\mathcal{L}_{rew}(\theta) + \mathcal{L}_{con}(\theta) + \mathcal{L}_{rec}(\theta) + \mathcal{L}_{obs}(\theta)
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Experiments Overview}
    \begin{itemize}
        \item \textbf{Key Research Questions:}
        \begin{itemize}
            \item Can APE improve learning speed and visual generalization?
            \item Does it work for both model-based and model-free RL?
            \item What makes APE effective and how sensitive is it to choices?
        \end{itemize}
        
        \item \textbf{Pretraining Setup:}
        \begin{itemize}
            \item Self-supervised on ImageNet-100
            \item ResNet-18 backbone
            \item Five augmentation primitives:
            \begin{itemize}
                \item Color Jitter, Grayscale, Gaussian Blur
                \item Random Resized Crop, Horizontal Flip
            \end{itemize}
            \item Dynamic re-weighting based on difficulty
            \item Gaussian blur as main augmentation ($f_{\rm main}$)
        \end{itemize}
        
        \item \textbf{Evaluation:}
        \begin{itemize}
            \item Linear probe accuracy on ImageNet-100:
            \begin{itemize}
                \item MoCo v2: 90.8\%
                \item APE (jitter): 91.1\%
                \item APE (blur): 91.7\% (best)
            \end{itemize}
            \item Consistent hyperparameters across all experiments
            \item Results averaged over $\geq$ 3 runs
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{DMC results}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.9\textwidth]{figs/loss_1.pdf}
        \caption{Loss comparison between DreamerV3, encoder with frozen random initialized parameters, encoder with trainable random initialized parameters and APE. The last layer of the frozen random initialized encoder is finetuned during training. The absolute value of actor loss is used.}
        \label{fig:loss}
    \end{figure}

\begin{itemize}
    \item \textbf{Starting Strong:} APE's pretrained encoder helps the model learn faster by giving it good initial features
    \item \textbf{Working Together:} When different parts of the model share good features, they all learn better and more steadily
    \item \textbf{Better Search:} Better features help the agent explore more of its environment, which is important for harder tasks
    \item \textbf{Overall:} Pretraining the vision model helps the agent learn faster and perform better on complex tasks
\end{itemize}
\end{frame}

\begin{frame}{Results on Other Benchmarks}
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{figs/atari1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
        \caption{Training curves for Atari 100k benchmarks.}
        \label{fig:Atari}
    \end{figure}
        
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.4\columnwidth]{figs/mem_1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
        \caption{Training curves for Memory Maze benchmarks. }
        \label{fig:mem}
    \end{figure}
        
    \begin{itemize}
        \item \textbf{Robustness:} The encoder we trained before works right away on Atari games without needing to change it for each game
        \item \textbf{Generalization:} What we learned from real photos (ImageNet-100) still helps with Atari's simple game graphics because we can change how we process the images
    \end{itemize}
\end{frame}

\begin{frame}{Comparison with Other Pretrained Algorithms (1/2)}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.6\columnwidth]{figs/pieg1.pdf} 
        \caption{Comparison of DreamerV3-based and DrQ-v2-based APE against other ResNet pretrained algorithms.}
        \label{figpieg}
    \end{figure}
\end{frame}

\begin{frame}{Comparison with Other Pretrained Algorithms (2/2)}
    Models:
    \begin{itemize}
        \item \textbf{SAC:vision} and \textbf{DrQ-v2} are our "pure pixel" baselines—model-free methods that learn directly from raw images
        \item \textbf{RRL} and \textbf{PIE-G} are two prior works that take a ResNet pretrained on ImageNet (or similar) and plug it into an RL pipeline, either freezing or fine-tuning it
        \item \textbf{APE (DreamerV3)} is our model-based variant (main text)
        \item \textbf{APE (DrQ-v2)} is an additional run we did to confirm that any downstream algorithm—model-based or model-free—can benefit from APE's pretrained encoder
        \item \textbf{SAC:state} is an oracle agent that learns from the simulator's true, low-dimensional state (not pixels)—this represents a strong upper bound on sample efficiency
    \end{itemize}

    Results:
    \begin{itemize}
        \item \textbf{Performance at 100K/500K steps:}
        \begin{itemize}
            \item APE significantly outperforms RRL, PIE-G, and pixel methods
            \item At 100K steps, APE nearly matches SAC:state performance
            \item By 500K steps, APE's advantage grows while pixel methods remain limited by poor features
        \end{itemize}
        
        \item \textbf{Works Everywhere:}
        \begin{itemize}
            \item APE works well with both model-free (DrQ-v2) and model-based (DreamerV3) methods
            \item This shows that our pretraining approach is flexible and not tied to any specific learning method
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Pretraining Does Work}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.9\columnwidth]{figs/random1.pdf} 
        \caption{Choosing suitable pretraining strategy weighs more than increasing the depth of encoder network. We compare APE with random initialized encoder with frozen parameters, random initialized encoder with trainable parameters and DreamerV3. The last layer of the frozen random initialized encoder is finetuned during training. ‘-18’ and ‘-4’ denote the number of layers used in the encoder.}
        \label{fig:init random}
        \end{figure}
\end{frame}

\begin{frame}{Augmentations Matter}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.9\columnwidth]{figs/aug1.pdf} 
        \caption{Different choices of augmentation strategy. APE with random gaussian blur as its main augmentation strategy outperforms other settings.}
        \label{fig:aug}
    \end{figure}
\end{frame}

\begin{frame}{Different Choices of Network Architectures}
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figs/arc1.pdf} 
\caption{Different choices of network architectures. This figure indicates that APE with ResNet18 achieves better results compared with a deeper APE (ResNet50).}
\label{fig:arc}
\end{figure}
\end{frame}




\end{document}