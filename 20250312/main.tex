\documentclass[9pt]{beamer}
\input{commands}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}

% TikZ and related libraries
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
% \usepackage{siunitx}
\usepackage{makecell}

\usepackage[sort]{natbib}

\AtBeginDocument{\RenewCommandCopy\qty\SI}

\pgfplotsset{compat=newest}

\newcommand{\blap}[1]{\vbox to 0pt{\hbox{#1}\vss}}

\newcommand{\enc}{h}
\newcommand{\Infop}{\info_{p,h}}
\newcommand{\Expp}{\Exp_{p,h}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\scoref}{\nabla_\x \log p^\tau(\x)}
\newcommand{\scorem}{\mathbf{S}_\theta(\x, \tau)}
\newcommand{\bbe}{\mathbb{E}}
\newcommand{\Tau}{\mathcal{T}}

\setbeamertemplate{navigation symbols}{}

% Add page number to the lower right corner
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}

% Customize the page number appearance
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}

\title{STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning}
\setbeamerfont{title}{size=\huge}
\date{March 12, 2025}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    
    \begin{itemize}
        \item Previous world model approaches:
        \begin{itemize}
            \item SimPLe: Uses LSTM
            \item DreamerV3: Employs GRU
            \item RNNs excel at sequence modeling but lack parallel computing capabilities
        \end{itemize}
        
        \item Transformer architecture advantages:
        \begin{itemize}
            \item Superior performance over RNNs
            \item Better handles long-term dependencies
            \item Enables efficient parallel computing
        \end{itemize}
        
        \item STORM (Stochastic Transformer-based wORld Model):
        \begin{itemize}
            \item Uses categorical VAE as image encoder
            \item Incorporates Transformer for sequence modeling
            \item Achieves 126.7\% mean human normalized score on Atari 100k
            \item Training requires only 4.3 hours on RTX 3090
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Comparison with Recent Approaches}

    Comparison between STORM and recent approaches. "Tokens" refers to the input tokens introduced to the sequence model during a single timestep. "Historical information" indicates whether the VAE reconstruction process incorporates historical data, such as the hidden states of an RNN.
  
    \begin{center}
    \resizebox{\textwidth}{!}{\begin{tabular}{cccccc}
    \toprule
    Attributes & SimPLe & TWM & IRIS & DreamerV3 & STORM \\
    \midrule
    Sequence model & LSTM & Transformer-XL & Transformer & GRU & Transformer\\
    Tokens & Latent & Latent, action, reward & Latent($4 \times 4$) & Latent & Latent \\
    Latent representation & Binary-VAE & Categorical-VAE & VQ-VAE & Categorical-VAE & Categorical-VAE \\
    Historical information & Yes & No & Yes & Yes & No\\
    Agent state & Reconstructed image & Latent & Reconstructed image & Latent, hidden & Latent, hidden \\
    Agent training & PPO & As DreamerV2 & As DreamerV2 & DreamerV3 & As DreamerV3 \\
    \bottomrule
    \end{tabular}}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Detailed Comparison with Prior Work}
    
    \begin{itemize}
        \item SimPLe and Dreamer rely on RNN-based models, whereas STORM employs a GPT-like Transformer as the sequence model.
        \item In contrast to IRIS that employs multiple tokens, STORM utilizes a single stochastic latent variable to represent an image.
        \item STORM follows a vanilla Transformer structure, while TWM adopts a Transformer-XL structure.
        \item In the sequence model of STORM, an observation and an action are fused into a single token, whereas TWM treats observation, action, and reward as three separate tokens of equal importance.
        \item Unlike Dreamer and TransDreamer, which incorporate hidden states, STORM reconstructs the original image without utilizing this information.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{STORM (\cite{zhang2023stormefficientstochastictransformer}) Architecture Overview}
    
    \begin{center}
        \includegraphics[width=\textwidth]{world_model_full.pdf}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Data Structure and Training Process}
    
    \begin{itemize}
        \item At each timestep $t$, a data point consists of:
        \begin{itemize}
            \item Observation $o_t$
            \item Action $a_t$
            \item Reward $r_t$
            \item Continuation flag $c_t$ (indicates if episode is ongoing)
        \end{itemize}
        \item Replay buffer:
        \begin{itemize}
            \item Maintains first-in-first-out queue structure
            \item Enables sampling of consecutive trajectories
        \end{itemize}
        \item Training process:
        \begin{enumerate}
        \item[S1] Execute policy to gather environment data into replay buffer
        \item[S2] Update world model using sampled trajectories
        \item[S3] Improve policy using imagined experiences from world model
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Model Structure: VAE Formulation}
    
    \begin{itemize}
        \item Convert image observations to latent space:
        \begin{align*}
            \text{Encoder: } & z_t \sim q_\phi(z_t|o_t) = \mathcal{Z}_t \\
            \text{Decoder: } & \hat{o}_t = p_\phi(z_t)
        \end{align*}
        
        \item Latent structure $\mathcal{Z}_t$:
        \begin{itemize}
            \item Stochastic categorical distribution
            \item 32 categories Ã— 32 classes
            \item Sampled variable $z_t \sim \mathcal{Z}_t$ represents $o_t$
        \end{itemize}
        
        \item Implementation details:
        \begin{itemize}
            \item CNN-based encoder $q_{\phi}$ and decoder $p_{\phi}$
            \item Straight-through gradient estimation for backpropagation
        \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Model Structure: Sequence Model}
    
    \begin{align*}
        \text{Action mixer: } & e_t = m_\phi(z_t,a_t) \\
        \text{Sequence model: } & h_{1:T} = f_\phi(e_{1:T}) \\
        \text{Dynamics predictor: } & \hat{\mathcal{Z}}_{t+1} = g^D_\phi(\hat{z}_{t+1}|h_t) \\
        \text{Reward predictor: } & \hat{r}_t = g^R_\phi(h_t) \\
        \text{Continuation predictor: } & \hat{c}_t = g^C_\phi(h_t)
    \end{align*}

    \begin{itemize}
        \item Model components and their roles:
        \begin{itemize}
            \item $m_\phi$: Combines latent states and actions into unified tokens through MLP-based fusion
            \item $f_\phi$: Processes temporal dependencies using a GPT-style Transformer with causal masking
            \item $g^D_\phi$: Predicts the next latent distribution to capture environment dynamics
            \item $g^R_\phi$: Estimates immediate rewards based on current hidden states
            \item $g^C_\phi$: Determines episode termination probability from current context
        \end{itemize}
        \item Key feature: Each token $e_t$ can only attend to its causal history ${e_1,e_2,\dots,e_t}$
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Model Structure: Loss Functions}
    
    \begin{itemize}
        \item Total loss function with hyperparameters $\beta_1=0.5$, $\beta_2=0.1$:

        \[
            \mathcal{L}(\phi) = \frac{1}{BT}\sum_{n=1}^{B}\sum_{t=1}^{T}\Bigl[\mathcal{L}^{\rm rec}_t (\phi) + \mathcal{L}^{\rm rew}_t (\phi) + \mathcal{L}^{\rm con}_t (\phi) + \beta_1 \mathcal{L}^{\rm dyn}_t (\phi) + \beta_2 \mathcal{L}^{\rm rep}_t (\phi)\Bigr]
        \]
        
        \item Environment prediction losses:
        \begin{itemize}
            \item $\mathcal{L}^{\rm rec}_t(\phi) = ||\hat{o}_t - o_t||_2$ % Image reconstruction loss using L2 norm
            \item $\mathcal{L}^{\rm rew}_t(\phi) = \mathcal{L}^{\rm sym}(\hat{r}_t, r_t)$ % Reward prediction with symlog two-hot loss
            \item $\mathcal{L}^{\rm con}_t(\phi) = c_t \log \hat{c}_t + (1 - c_t) \log (1 - \hat{c}_t)$ % Binary cross-entropy for continuation prediction
        \end{itemize}
        
        \item Latent dynamics losses (KL-divergence based):
        \begin{itemize}
            \item $\mathcal{L}^{\rm dyn}_t(\phi) = \max\bigl(1, \mathrm{KL}\bigl[\mathrm{sg}(q_\phi(z_{t+1}|o_{t+1}))\ ||\  g^D_\phi(\hat{z}_{t+1}|h_t) \bigr]\bigr)$ % Trains dynamics predictor to match encoder distribution (encoder gradients stopped)
            \item $\mathcal{L}^{\rm rep}_t(\phi) = \max\bigl(1, \mathrm{KL}\bigl[q_\phi(z_{t+1}|o_{t+1})\ ||\ \mathrm{sg}( g^D_\phi(\hat{z}_{t+1}|h_t)) \bigr]\bigr)$ % Trains encoder to match dynamics predictor distribution (predictor gradients stopped)
            \item Both losses are KL divergences clamped to minimum value of 1 with selective gradient stopping
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Agent Learning}
    
    \begin{itemize}
        \item During inference:
        \begin{itemize}
            \item Sample $z_t$ from prior $\hat{\mathcal{Z}}_t$ rather than posterior $\mathcal{Z}_t$
            \item Use KV cache in Transformer for faster inference
        \end{itemize}
        
        \item Agent state and policy:
        \begin{align*}
            \text{State: } & s_t = [z_t, h_t] \\
            \text{Critic: } & V_\psi(s_t) \approx \mathbb{E}_{\pi_\theta, p_\phi}\Bigl[\sum_{k=0}^\infty \gamma^k r_{t+k}\Bigr] \\
            \text{Actor: } & a_t \sim \pi_\theta(a_t|s_t)
        \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Agent Learning: Actor-Critic Algorithm}
    
    \begin{itemize}
        \item Actor-critic loss functions (from DreamerV3):
        \begin{subequations} \label{eq:reinforce}
        \begin{align}
        \label{eq:reinforce_actor}
        \mathcal{L}(\theta)& = \frac{1}{BL}\sum_{n=1}^{B}\sum_{t=1}^{L}\biggl[-\mathrm{sg}\bigg(\frac{G^\lambda_{t}- V_{\psi}(s_{t})}{\max(1, S)}\bigg) \ln \pi_\theta(a_t|s_t) -\eta H\bigl(\pi_\theta(a_t|s_t)\bigr)\biggr]\\
        \label{eq:reinforce_critic}
        \mathcal{L}(\psi)& =  \frac{1}{BL}\sum_{n=1}^{B}\sum_{t=1}^{L}\biggl[\Bigl(V_\psi(s_t) - \mathrm{sg}\bigl(G^\lambda_{t}\bigr)\Bigr)^2 + \Bigl(V_\psi(s_t) - \mathrm{sg}\bigl(V_{\psi^{\mathrm{EMA}}}(s_t)\bigr)\Bigr)^2\biggr]
        \end{align}
        \end{subequations}
        
        \item $\lambda$-return calculation:
        \begin{subequations}
        \label{eq:lambda_return}
        \begin{align}
            G^\lambda_{t} &\doteq r_t + \gamma c_t \Bigl[(1-\lambda) V_{\psi}(s_{t+1}) + \lambda G^{\lambda}_{t+1}\Bigr] \\
            G^\lambda_{L} &\doteq V_{\psi}(s_{L})
        \end{align}
        \end{subequations}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Agent Learning: Training Details}
    
    \begin{itemize}
        \item Normalization ratio $S$ for actor loss:
        \begin{equation} \label{eq:norm_lambda_return}
        \begin{gathered}
        S = \mathrm{percentile}(G^\lambda_{t}, 95) - \mathrm{percentile}(G^\lambda_{t}, 5)
        \end{gathered}
        \end{equation}
        
        \item Value function regularization using EMA:
        \begin{equation} \label{eq:slow_critic}
        \psi^{\mathrm{EMA}}_{t+1} = \sigma \psi^{\mathrm{EMA}}_t + (1-\sigma) \psi_{t}
        \end{equation}
        where:
        \begin{itemize}
            \item $\sigma$ is the decay rate
            \item $\psi_t$ represents current critic parameters
            \item $\psi^{\mathrm{EMA}}_{t+1}$ denotes updated critic parameters
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Game Scores}
    \begin{table}[h]  
        \centering
        \resizebox{0.9\textwidth}{!}{\begin{tabular}{lrrrrrrr}
          \toprule
          Game & Random & Human & SimPLe & TWM & IRIS & DreamerV3 & STORM \\
          \midrule
          Alien          & 228    & 7128   & 617    & 675    & 420    & \textbf{959}    & \textbf{984} \\
          Amidar         & 6      & 1720   & 74     & 122    & 143    & 139    & \textbf{205} \\
          Assault        & 222    & 742    & 527    & 683    & \textbf{1524}   & 706    & 801 \\
          Asterix        & 210    & 8503   & \textbf{1128}   & \textbf{1116}   & 854    & 932    & 1028 \\
          Bank Heist     & 14     & 753    & 34     & 467    & 53     & \textbf{649}    & \textbf{641} \\
          Battle Zone    & 2360   & 37188  & 4031   & 5068   & \textbf{13074}  & 12250  & \textbf{13540} \\
          Boxing         & 0      & 12     & 8      & \textbf{78}     & 70     & \textbf{78}     & \textbf{80} \\
          Breakout       & 2      & 30     & 16     & 20     & \textbf{84}     & 31     & 16 \\
          Chopper Command   & 811    & 7388   & 979    & 1697   & 1565   & 420    & \textbf{1888} \\
          Crazy Climber  & 10780  & 35829  & 62584  & 71820  & 59234  & \textbf{97190}  & 66776 \\
          Demon Attack   & 152    & 1971   & 208    & 350    & \textbf{2034}   & 303    & 165 \\
          Freeway        & 0      & 30     & 17     & 24     & \textbf{31}     & 0      & \textbf{34} \\
          Freeway w/o traj& 0     & 30     & 17     & 24     & \textbf{31}     & 0      & 0 \\
          Frostbite      & 65     & 4335   & 237    & \textbf{1476}   & 259    & 909    & 1316 \\
          Gopher         & 258    & 2413   & 597    & 1675   & 2236   & 3730   & \textbf{8240} \\
          Hero           & 1027   & 30826  & 2657   & 7254   & 7037   & \textbf{11161}  & \textbf{11044} \\
          James Bond     & 29     & 303    & 101    & 362    & 463    & 445    & \textbf{509} \\
          Kangaroo       & 52     & 3035   & 51     & 1240   & 838    & \textbf{4098}   & \textbf{4208} \\
          Krull          & 1598   & 2666   & 2204   & 6349   & 6616   & 7782   & \textbf{8413} \\
          Kung Fu Master & 256    & 22736  & 14862  & 24555  & 21760  & 21420  & \textbf{26182} \\
          Ms Pacman      & 307    & 6952   & 1480   & 1588   & 999    & 1327   & \textbf{2673} \\
          Pong           & -21    & 15     & 13     & \textbf{19}     & 15     & \textbf{18}     & 11 \\
          Private Eye    & 25     & 69571  & 35     & 87     & 100    & 882    & \textbf{7781} \\
          Qbert          & 164    & 13455  & 1289   & 3331   & 746    & 3405   & \textbf{4522} \\
          Road Runner    & 12     & 7845   & 5641   & 9109   & 9615   & 15565  & \textbf{17564} \\
          Seaquest       & 68     & 42055  & 683    & \textbf{774}    & 661    & 618    & 525 \\
          Up N Down      & 533    & 11693  & 3350   & \textbf{15982}  & 3546   & 7667   & 7985 \\
          \midrule
          Human Mean     & 0\%    & 100\%  & 33\% & 96\% & 105\%  & 112\%   & \textbf{126.7}\% \\
          Human Median   & 0\%    & 100\%  & 13\% & 51\% & 29\%   & 49\%    & \textbf{58.4}\% \\
          \bottomrule
        \end{tabular}}
      \end{table}      
    \end{frame}

\begin{frame}
    \frametitle{World Model Architecture Variants}
    \begin{itemize}
        \item Two key architectural variants explored:
        \begin{itemize}
            \item ``Decoder at rear'': Using $z_t \sim \hat{\mathcal{Z}}_t$ instead of $z_t \sim \mathcal{Z}_t$ for observation reconstruction. It shows that reconstruction loss should be applied directly to encoder output
            \item ``Predictor at front'': Using $z_t$ instead of $h_t$ as input for prediction functions - minimal impact on single-frame reward tasks (e.g. \textit{Pong}), but performance drops on multi-frame reward tasks (e.g. \textit{Ms. Pacman})
        \end{itemize}
        
        \vspace{2.5em}

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.7\textwidth]{ablations_rear_front.pdf}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Model Size and Layer Ablation Study}
    \begin{itemize}
        \item Default configuration uses 2 Transformer layers
        \begin{itemize}
            \item Much smaller than 10-layer models in IRIS and TWM
        \end{itemize}
        
        \item Layer ablation study findings:
        \begin{itemize}
            \item More layers do not improve performance
            \item In \textit{Pong}, 4 and 6-layer models reach max reward even with 4x samples
        \end{itemize}
        
        \item Three key factors explaining this scaling behavior:
        \begin{itemize}
            \item Frame prediction is simplified by:
            \begin{itemize}
                \item Small frame-to-frame differences, residual connections aiding prediction
            \end{itemize}
            \item Limited data in Atari 100k:
            \begin{itemize}
                \item Lacks image diversity, therefore insufficient samples for larger models
            \end{itemize}
            \item End-to-end training effects:
            \begin{itemize}
                \item \(\mathcal{L}^{\rm rep}\) loss directly impacts encoder: Large sequence models may overly influence encoder
            \end{itemize}
        \end{itemize}

        \vspace{1em}
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.7\textwidth]{ablations_layers.pdf}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Agent State Representation Study}
    \begin{itemize}
        \item Options for agent's state $s_t$ include:
        \begin{itemize}
            \item Predicted observation $\hat{o}_t$, hidden state $h_t$, latent state $z_t$, or combined state $[h_t, z_t]$
        \end{itemize}
        
        \item Key findings from ablation studies:
        \begin{itemize}
            \item Context-dependent performance:
            \begin{itemize}
                \item Including $h_t$ improves performance in context-heavy games (e.g. \textit{Ms. Pacman})
                \item Minimal impact in simpler games like \textit{Pong}, \textit{Kung Fu Master}
            \end{itemize}
            
            \item Limitations of using only $h_t$:
            \begin{itemize}
                \item Can lead to catastrophic forgetting in policy-dependent environments
                \item World model's non-stationarity creates instability
            \end{itemize}
            
            \item Benefits of randomness:
            \begin{itemize}
                \item Including distributions like $\mathcal{Z}_t$ helps mitigate $h_t$-only issues
            \end{itemize}
        \end{itemize}

        \vspace{1em}
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{ablations_state.pdf}
        \end{figure}
    \end{itemize}
\end{frame}


\end{document}