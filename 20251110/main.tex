\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{comment} % For \begin{comment} and \end{comment}
\usepackage{mathtools} % For \coloneqq
\usepackage{bm} % For bold math symbols
\input{math_commands.tex}

\title{MuDreamer: Learning Control-Focused World Models without Pixel Reconstruction}
\author{Withdraw From ICLR 2024}
\date{November 10, 2025}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Problem Setup in DreamerV3}
    \textbf{DreamerV3 trains latents by reconstructing pixels.}
    \begin{itemize}
        \item Pixel reconstruction shapes the representation toward \textit{all} visual details (incl. backgrounds).
        \item Under strong visual distractors, this can dilute task-relevant features.
        \item Reconstruction is compute-heavy and not strictly necessary for control.
    \end{itemize}
    \textbf{Goal:} Focus the learned latent state ( $\bm{s}_t = \{h_t, z_t\}$ ) on \textit{control-relevant} information (rewards, continuation, value, action effects), not textures.
\end{frame}

\begin{frame}
    \frametitle{MuDreamer’s Core Idea}
    \textbf{Drop pixel reconstruction as a shaping signal.}
    Learn a latent world model by predicting \textit{task-relevant} quantities only:
    \begin{itemize}
        \item Reward ($\hat{r}_t$) and Continue ($\hat{c}_t$)
        \item \textbf{Value} (distributional, discretized ($\lambda$)-return)
        \item \textbf{Past Action} (inverse dynamics)
    \end{itemize}
    Optional: keep a decoder for visualization \textbf{with stop-grad}, so it does not influence latents.

    \textbf{Why:} Dense supervision each step, but aligned with \textit{control}.
\end{frame}

\begin{frame}
    \frametitle{Architecture at a Glance}
    \textbf{Per-timestep (strictly sequential)}
    \begin{enumerate}
        \item Update memory: ($h_t = f_\phi(h_{t-1}, z_{t-1}, a_{t-1})$)
        \item Prior over state: ($\hat{z}_t \sim p_\phi(z\mid h_t)$)
        \item Encode image: ($x_t = \mathrm{enc}_\phi(o_t)$)
        \item Posterior: ($z_t \sim q_\phi(z\mid h_t, x_t)$)
        \item Bundle: ($s_t = \{h_t, z_t\}$)
        \item Heads: ($\hat{r}_t, \hat{c}_t, \hat{v}_t$) from ($s_t$); \textbf{($\widehat{a}_{t-1}$)} from (($x_t, s_{t-1}$)); optional decoder ($\hat{o}_t$) from ($\mathrm{sg}(s_t)$).
    \end{enumerate}
    \textbf{Normalization:} Use BatchNorm in the representation net to avoid collapse.
\end{frame}

\begin{frame}
    \frametitle{Architecture Diagram}
    \centering
    \includegraphics[width=1.1\textwidth]{fig/archtitechure.png}
\end{frame}

\begin{frame}
    \frametitle{Loss Stack (World Model)}
    Total model loss (sequence length ($T$)):
    \[
    \mathcal{L}_{\text{model}} = \sum_{t=1}^{T}\Big(\beta_{\text{pred}}\mathcal{L}_{\text{pred},t}+\beta_{\text{dyn}}\mathcal{L}_{\text{dyn},t}+\beta_{\text{rep}}\mathcal{L}_{\text{rep},t}\Big).
    \]
    \textbf{Prediction loss ($\mathcal{L}_{\text{pred}}$)} trains:
    \begin{itemize}
        \item Reward: ($-\log p_\phi(r_t\mid s_t)$)
        \item Continue: ($-\log p_\phi(c_t\mid s_t)$)
        \item \textbf{Value}: ($-\log p_\phi(R_t^{\lambda}\mid s_t)$) (two-hot symlog discretization)
        \item \textbf{Past Action}: ($-\log p_\phi(a_{t-1}\mid x_t, s_{t-1})$)
        \item Optional reconstruction (aux): ($-\log p_\phi(o_t\mid \mathrm{sg}(s_t))$)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Loss Stack (KL Terms)}
    \textbf{Dynamics KL (train prior to match posterior)}
    \[
    \mathcal{L}_{\text{dyn},t} = \max\big(1, \mathrm{KL}[\mathrm{sg}q_\phi(z_t\mid h_t,x_t)\mid p_\phi(z\mid h_t)]\big).
    \]
    \textbf{Representation KL (train posterior to be predictable)}
    \[
    \mathcal{L}_{\text{rep},t} = \max\big(1, \mathrm{KL}[q_\phi(z_t\mid h_t,x_t)\mid \mathrm{sg}p_\phi(z\mid h_t)]\big).
    \]
    \textbf{Typical weights:} ($\beta_{\text{pred}}=1.0, \beta_{\text{dyn}}=0.95, \beta_{\text{rep}}=0.05$).

    \textbf{Note:} KLs are capped below at 1 for stability.
\end{frame}

\begin{frame}
    \frametitle{Value Head Target (on Real Sequences)}
    Use a slow \textbf{EMA teacher} ($v_{\phi'}$) to define ($\lambda$)-return targets:
    \[
    R_t^{\lambda}= r_{t+1}+\gamma c_{t+1}\big((1-\lambda)v_{\phi'}(s_{t+1})+\lambda R_{t+1}^{\lambda}\big),\quad R_T^{\lambda}=v_{\phi'}(s_T).
    \]
    \begin{itemize}
        \item Transform with symlog, discretize with two-hot bins; train the student value head ($v_\phi$) with cross-entropy.
        \item Purpose: shape \textbf{representations} (not used for behavior learning on imagined rollouts).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Behavior Learning (Dreamer-style)}
    \textbf{Imagination in latent space (unchanged from V3):}
    \begin{itemize}
        \item Start from a posterior state, roll out ($H=15$) steps with the \textbf{prior} and the \textbf{actor}.
        \item Use model heads ($\hat{r}, \hat{c}$) on imagined states to construct discretized ($\lambda$)-return targets.
        \item \textbf{Critic} ($v_\psi$) learns on imagined trajectories, with EMA regularizer toward ($v_{\psi'}$).
        \item \textbf{Actor} maximizes normalized advantages plus entropy; backprop-through-model (continuous) or REINFORCE (discrete).
    \end{itemize}
    \textbf{Key separation:} behavior uses ($v_\psi$), not the world-model ($v_\phi$).
\end{frame}

\begin{frame}
    \frametitle{Why Past-Action \& Value Heads?}
    \begin{itemize}
        \item \textbf{Value head (on real data):} forces latents to encode \textit{long-horizon, return-relevant} features; improves stability.
        \item \textbf{Past-action head:} inverse dynamics from (($x_t, s_{t-1}$)) $\to$ ($a_{t-1}$) gives dense, control-aligned supervision each step, especially when rewards are sparse.
        \item Together they replace pixel reconstruction as a representation-learning signal, while keeping behavior learning unchanged.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results: DeepMind Visual Control Suite (1M steps)}
    \begin{itemize}
        \item Competitive or better than DreamerV3 on several tasks (e.g., Cheetah Run, Quadruped Walk, Reacher Hard).
        \item Overall mean/median scores comparable or improved with faster convergence in many cases.
        \item Long-horizon latent predictions remain accurate without reconstruction gradients.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{DeepMind Visual Control Suite Results}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/visiual_control_suite_result_table.png}
\end{frame}

\begin{frame}
    \frametitle{Results: Natural Background (Robustness to Visual Distractors)}
    \begin{itemize}
        \item Natural-video backgrounds make pixel reconstruction overly track textures.
        \item MuDreamer’s aux decoder reconstructions visually \textbf{filter irrelevant background}, evidence the latent ignores distractors.
        \item Reported mean/median (example): Mean $\approx$ 517, Median $\approx$ 620 (vs. Dreamer baselines under same setting).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results: Atari}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/atari_result_table.png}
\end{frame}

\begin{frame}
    \frametitle{Ablations}
    \textbf{Action \& Value Heads}
    \begin{itemize}
        \item Remove \textbf{Action} $\to$ performance drops (notably on sparse-reward tasks like Hopper Hop, Cartpole Swingup Sparse).
        \item Remove \textbf{Value} $\to$ less stable training.
        \item Remove \textbf{Both} $\to$ larger drop.
    \end{itemize}
    \textbf{Conclusion:} Both heads help the model learn dynamics-aware, control-relevant latents.

    \vspace{1em}

    \textbf{BatchNorm \& KL Balancing}
    \begin{itemize}
        \item \textbf{BatchNorm in representation} prevents collapse without pixel reconstruction; improves stability and speed.
        \item \textbf{KL balance} (($\beta_{\text{rep}}$) vs. ($\beta_{\text{dyn}}$)) matters: too little rep-KL destabilizes; too much slows learning.
        \item Reported curves (mean score across 20 DMC-Visual tasks) highlight sensitivity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementation Notes}
    \begin{itemize}
        \item \textbf{Horizon:} ($H=15$). \textbf{Discount/Trace:} ($\gamma=0.997$), ($\lambda=0.95$).
        \item \textbf{EMA teacher for ($v_\phi$):} momentum ($\tau=0.01$).
        \item \textbf{Weights:} ($\beta_{\text{pred}}=1.0$), ($\beta_{\text{dyn}}=0.95$), ($\beta_{\text{rep}}=0.05$).
        \item \textbf{Normalization:} BatchNorm in representation net.
        \item \textbf{Heads:} reward/continue/value from ($s_t$); past-action from (($x_t, s_{t-1}$)).
        \item \textbf{Optional decoder:} stop-grad only (visualization).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{My Skepticism (Failure Modes)}
    \begin{itemize}
        \item \textbf{Action–state ambiguity:} inverse dynamics may be ill-posed; weak learning signal.
        \item \textbf{Partial observability/occlusion:} ($x_t$) may not reflect ($a_{t-1}$)’s effect.
        \item \textbf{Shortcut risks:} leaks if using ($s_t$) as input; overfit to trivial cues without strong data augs.
        \item \textbf{Non-controllable but relevant factors:} inverse dynamics won’t emphasize them; value head helps but not always enough.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What I’d Test or Swap In}
    \begin{itemize}
        \item \textbf{Stress tests:} sparse reward + distractors; measure linear probes for velocities/contacts; MI (I($a_{t-1}; x_t\mid s_{t-1}$)); rollout error.
        \item \textbf{Alternatives:} action-conditioned contrastive (InfoNCE), bisimulation-style regularization, forward-consistency KL/JSD, controllability Jacobian regularizers.
    \end{itemize}
    \textbf{Takeaway:} Treat past-action \& value heads as \textit{toggleable biases toward controllability and long-horizon relevance}; keep them if ablations pay off.
\end{frame}

\end{document}