\documentclass[9pt]{beamer}
\input{commands.tex}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}

% TikZ and related libraries
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
% \usepackage{siunitx}
\usepackage{makecell}

\usepackage[sort]{natbib}

% \input{math_commands.tex}

\AtBeginDocument{\RenewCommandCopy\qty\SI}

\pgfplotsset{compat=newest}

\setbeamertemplate{navigation symbols}{}

% Add page number to the lower right corner
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}

% Customize the page number appearance
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}

\title{Simulus, SGF}
\setbeamerfont{title}{size=\huge}
\date{July 2, 2025}

\begin{document}

\begin{frame}
    \titlepage % This command creates the title page
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}


\section{Uncovering Untapped Potential in Sample-Efficient World Model Agents (Simulus)}

\begin{frame}
    \begin{center}
        \usebeamerfont{section title} \insertsection
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Simulus \citep{cohen2025uncoveringuntappedpotentialsampleefficient}}

    \begin{itemize}
        \item \textbf{Simulus} builds on top of REM and following the same \( \mathcal{V} \)-\(\mathcal{M} \)-\(\mathcal{C}  \) structure.
        \item Tokenizer \( \mathcal{V} \) encodes raw observations \( o_t \) and actions \( a_t \) into \emph{fixed-length} sequence.
        \item World model \( \mathcal{M} \) parameterized by \( \theta \), would embed and stream into a \emph{RetNet} world model \( f_\theta \). (POP lets the RetNet predict the whole next observation in one go instead of token-by-token.)
        \item An ensemble of 4 identical heads predicts the same tokens. Their Jensen-Shannon disagreement is the intrinsic reward \( i_t \).
        \item A LSTM Actor-Critic \( \mathcal{C} \) is optimized with:
        \[
            \bar r_t = \alpha_{\text{ext}} \hat r_t + \alpha_{\text{int}} i_t .
        \]
        \item Prioritized WM replay: world-model batches are 30\% high-loss frames, 70\% uniform.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Tokenizer \( \mathcal V \)}

    \begin{itemize}
        \item Each modality would have its own encoder/decoder pair.
    \end{itemize}

\begin{table}[h]
    \centering
    \scalebox{0.77}{
    \begin{tabular}{l|l|c|c}
        \textbf{Modality} & \textbf{Encoder} & \textbf{Tokens per obs} & \textbf{Vocab} \\
        \hline
        Image $64\times64$ & 3-level VQ-VAE & $8\times8=64$ & 512 \\
        Vector (cont.) & Scalar $\rightarrow$ 125-bin quantiser $\pm$symlog & len(vector) & 125 \\
        Categorical & identity (already integer) & len(cat) & native \\
        2-D grid & flatten, embed \& average & $m n$ & sizes per channel \\
    \end{tabular}
    }
    \caption{Tokenizer modalities and their encoding specifications}
\end{table}
\end{frame}

\begin{frame}
    \frametitle{World Model \( \mathcal M \)}

    World Model input:
    \begin{itemize}
        \item Tokenizer converts raw observations \( o_t \) into tokens \( z_t = (z_{t,1}, \ldots, z_{t,K}) \).
        \item Each token \( z_{t,j} \) is a small integer \( (0 \ldots \text{vocab\_size}-1) \)
        \item Tokens are mapped to fixed-size embeddings \( e_{t,j} \in \mathbb{R}^d \)
        \item Concatenate \( K \) vectors to form observation block \( E_t = (e_{t,1}, \ldots, e_{t,K}) \)
        \item Action tokens \( A_t \) are appended to \( E_t \), forming input pair \( \underbrace{E_t}_{\text{observation}}, \underbrace{A_t}_{\text{action tokens}} \).
        \item The world model's input stream follows the pattern: \( E_1 A_1 \, E_2 A_2 \, E_3 A_3 \, \ldots \)
    \end{itemize}

    World Model architecture:
    \begin{itemize}
        \item \textbf{RetNet Architecture}: Transformer-like network where expensive self-attention is replaced by cheaper \textbf{Retention} operation
        \item \textbf{Sequential Operation}: At each step consumes one full $(E,A)$ block and updates hidden state:
        \[
            (h_t,x_t) = f_\theta\bigl(h_{t-1},\,(\mathbf E_t,\mathbf A_t)\bigr)
        \]
        where \( h_t \): recurrent state, \( x_t \): prediction of next observation.
        \item \textbf{POP}: Uses learnable query sequence $\mathbf U$ to predict all $K$ tokens in parallel
        \item \textbf{POP Implementation}:
        \begin{enumerate}
            \item After processing block $t$, hold hidden state $h_t$
            \item Call RetNet with learnable $\mathbf U$: $(\_,\,y^{u}_t) = f_\theta(h_t,\mathbf U)$
            \item Feed $y^{u}_t\in\mathbb R^{K\times d}$ rows into MLP heads to get $p_\theta(\hat z_{t+1,j}\,|\,y^{u}_{t,j})$
        \end{enumerate}
        \item \textbf{Result}: Parallel $K$ predictions enable $K\times$ faster generation
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Intrinsic Reward Signal}

    \textbf{Motivation:} Extrinsic reward is sparse early in training. Model-based agents can measure their own ignorance - prediction disagreements indicate where more data is needed.

    \textbf{Implementation:}
    \begin{itemize}
        \item \textbf{4 Independent Heads}: $p_{\phi_1}, p_{\phi_2}, p_{\phi_3}, p_{\phi_4}$ all predict from same RetNet
        \item \textbf{Jensen-Shannon Divergence (JSD)} measures disagreement:
        \[
            u_j = H\!\left(\tfrac14\sum_{i=1}^4 p_{\phi_i}\right) - \tfrac14\sum_{i=1}^4 H(p_{\phi_i})
        \]
        \item \textbf{Average over tokens}: $i_t = \frac1K \sum_{j=1}^{K} u_j$
        \item \textbf{Combined reward}: $\bar r_t = \alpha_{\text{ext}}\hat r_t + \alpha_{\text{int}} i_t$
    \end{itemize}

    \textbf{Architecture:}
    \begin{itemize}
        \item All heads see \textbf{stop-gradient} inputs for JSD computation
        \item JSD bounded in $[0,\log V]$ - mixes well with any reward scale
        \item High JSD frames get high intrinsic reward and prioritized replay
        \item Creates feedback loop: explore → learn → reduce uncertainty → shift exploration
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Prioritized Replay for World Model}

    \textbf{Implementation:}
    \begin{itemize}
        \item Store latest obs-loss with each transition
        \item WM batch sampling: 70\% uniform, 30\% softmax(loss)
        \item New frames: high dummy loss for guaranteed sampling
    \end{itemize}

    \textbf{Benefits:}
    \begin{itemize}
        \item Focus on hard-to-predict transitions
        \item New experiences always sampled
        \item Balanced learning via uniform component
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Controller \( \mathcal C \)}

    \textbf{Overview:} Outputs stochastic policy $\pi_\psi(a_t\mid\tau_t)$ and value estimate $V_\psi(\tau_t)$

    \textbf{Recurrent Backbone:}
    \begin{itemize}
        \item Single-layer LSTM
        \item $h_t,c_t = \text{LSTM}(v_t, h_{t-1},c_{t-1})$
        \item Constitutes majority of controller parameters $\psi$
    \end{itemize}

    \textbf{Input Pipeline:}
    \begin{itemize}
        \item \textbf{Modality Encoders}: Each token type $z^{(i)}_t$ gets encoded via $E^{(i)}:\{0,\dots,V_i-1\}^{K_i}\to\mathbb R^{d_i}$
        \item \textbf{Fusion MLP}: $v_t = g_{\text{fuse}}([v^{(0)}_t \| v^{(1)}_t \| \cdots]) \in\mathbb R^{d_{\text{fuse}}}$
    \end{itemize}

    \textbf{Training:}
    \begin{itemize}
        \item Generate imagined trajectory $\hat\tau = (z_1,a_1,\bar r_1,\dots,z_H,a_H,\bar r_H)$
        \item $\lambda$-returns + REINFORCE with value baseline + entropy regularization like Dreamer.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results}

    \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{figures/image.png}
        \caption{Simulus results}
    \end{figure}
\end{frame}

\section{Simple, Good, Fast: Self-Supervised World Models Free of Baggage (SGF)}

\begin{frame}
    \begin{center}
        \usebeamerfont{section title} \insertsection
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{SGF}

    \textbf{Aim:} Strip world-models to bare essentials - can we do well without RNNs/Transformers, discrete latents, or pixel reconstructions?

    \textbf{Core Ingredients:}
    \begin{itemize}
        \item Self-supervised representations (VICReg-style)
        \item Two losses: temporal consistency + information maximization
        \item Frame + action stacking (captures short-term dependencies)
        \item Strong image augmentations (injects stochasticity)
        \item Deterministic, feed-forward dynamics (MLP predicts $\Delta$-latent, reward, terminal)
    \end{itemize}

    \textbf{Why it works:}
    \begin{itemize}
        \item Learned latents are informative and locally smooth $\rightarrow$ simple dynamics suffices
        \item Augmentations stand in for stochastic modelling
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ingredients Leading to Simplicity}

    \textbf{Stacking instead of memory:}
    \begin{itemize}
        \item Traditional: RNNs/attention for long-term dependencies
        \item Their approach: \emph{frame and action stacking}
        \item Stack $m$ recent frames $\rightarrow$ captures velocity, partial observability
        \item Stack $m$ recent actions $\rightarrow$ handles action delays
        \item Much faster than recurrent/attention mechanisms
    \end{itemize}

    \textbf{Augmentations instead of stochasticity:}
    \begin{itemize}
        \item Prior models: stochastic even in deterministic POMDPs
        \item Our approach: \emph{data augmentation} for stochasticity
        \item Random augmentations during training $\rightarrow$ robustness
        \item Avoids computational overhead of stochastic predictions
        \item Similar to successful model-free approaches (DrQ, RAD)
    \end{itemize}

    \textbf{Key insight:} Simple alternatives achieve similar performance with much less complexity
\end{frame}

\begin{frame}
    \frametitle{Model Structure}

    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/image3.png}
        \caption{Model Structure}
    \end{figure}
    
\end{frame}

\begin{frame}
    \frametitle{Representation Learning}

    \begin{itemize}
        \item Given transition $(\mathbf{o}, \mathbf{a}, \mathbf{o}', r, e)$, apply random augmentations $t, t' \sim \mathcal{T}$ to get $\tilde{\mathbf{o}} = t(\mathbf{o})$ and $\tilde{\mathbf{o}}' = t'(\mathbf{o}')$
        \item Encoder $f_{\text{enc}}$ computes representations $\tilde{\mathbf{y}} = f_{\text{enc}}(\tilde{\mathbf{o}})$ and $\tilde{\mathbf{y}}' = f_{\text{enc}}(\tilde{\mathbf{o}}')$
        \item Projector $f_{\text{proj}}$ computes embeddings $\tilde{\mathbf{z}} = f_{\text{proj}}(\tilde{\mathbf{y}})$ and $\tilde{\mathbf{z}}' = f_{\text{proj}}(\tilde{\mathbf{y}}')$
        \item Action-conditioned predictor $f_{\text{pred}}$ predicts $\hat{\mathbf{z}}' = f_{\text{pred}}(\tilde{\mathbf{z}}, \mathbf{a})$
    \end{itemize}

    \textbf{Loss Function:}
    \[
    \mathcal{L}_\text{Repr.}(\theta) = \mathbb{E}_\tau \Big[ \underbrace{ \tfrac{\eta}{D} \Vert f_{\text{pred}}(\tilde{\mathbf{z}}, \mathbf{a}) - \tilde{\mathbf{z}}' \Vert_2^2}_\text{Temporal Consistency} + \underbrace{\vphantom{\tfrac{1}{d}} \text{VC}(\tilde{\mathbf{Z}}) + \text{VC}(\tilde{\mathbf{Z}}')}_\text{Information Maximization}\, \Big]
    \]

    \textbf{Variance-Covariance Regularization:}
    \[
    \text{VC}(\mathbf{Z}) = \frac{1}{D} \sum_{j=1}^{D} \bigg[ \underbrace{\rho \max\!\left( 0, 1 - \sqrt{\text{Cov}(\mathbf{Z})_{j,j} + \varepsilon} \right)}_\text{Variance} + \underbrace{\nu \sum\nolimits_{k \neq j} \text{Cov}(\mathbf{Z})_{j,k}^2}_\text{Covariance}\, \bigg]
    \]

    Where \( \mathbf{Z} \) is the set of all embeddings in the batch.

    \textbf{V}ariance regularization keeps the standard deviation of each embedding
    feature across the batch above $1$ using a hinge loss. \textbf{C}ovariance
    regularization decorrelates the embedding features by attracting their
    covariances towards zero
\end{frame}

\begin{frame}
    \frametitle{Dynamics Model}

    \textbf{Maximum likelihood estimation:}
    \[
    \mathcal{L}_\text{Dyn.}(\theta) = \mathbb{E}_\tau \Big[ - \underbrace{ \log p_\theta(\mathrm{sg}(\mathbf{y}') | \mathrm{sg}(\mathbf{y}), \mathbf{a}) }_\text{Transition Distribution} -\underbrace{\vphantom{\tfrac{1}{d}} \log p_\theta(r | \tilde{\mathbf{y}}, \mathbf{a}, \tilde{\mathbf{y}}')}_\text{Reward Distribution} {}-{} \underbrace{\vphantom{\tfrac{1}{d}} \log p_\theta(e | \tilde{\mathbf{y}}, \mathbf{a}, \tilde{\mathbf{y}}')}_\text{Terminal Distribution}\, \Big]
    \]

    \textbf{Key design choices:}
    \begin{itemize}
        \item Stop-gradient $\mathrm{sg}(\cdot)$ on representations in transition loss
        \item Prevents moving targets from representation model updates
        \item Rewards/terminals provide stable POMDP signals
        \item Learn transitions with non-augmented observations: $\mathbf{y} = f_{\mathrm{enc}}(\mathbf{o})$, $\mathbf{y}' = f_{\mathrm{enc}}(\mathbf{o}')$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Results}

    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/image2.png}
        \caption{Score and runtime comparison in the
        Atari 100k benchmark. SPR is model-free, EfficientZero performs lookahead.}
    \end{figure}
\end{frame}


\section{Limitations}

\begin{frame}
    \frametitle{Limitations}

    \textbf{1. Limited to MDPs, not POMDPs:}
    \begin{itemize}
        \item Only works with \textbf{deterministic MDPs}
        \item Cannot handle \textbf{non-deterministic POMDPs} because:
        \begin{itemize}
            \item Transition distribution needs to be \textbf{stochastic}
            \item Predictor must handle \textbf{uncertainty} between observations
            \item Both networks need \textbf{stochastic predictions} (mean/variance or Gaussian mixtures)
        \end{itemize}
        \item Limited to \textbf{short-term dependencies} (avoided RNN/Transformers)
        \item This might explain why we didn't reach \textbf{state-of-the-art performance}
    \end{itemize}

    \textbf{2. VICReg Image Requirement:}
    \begin{itemize}
        \item VICReg currently requires \textbf{image observations}
        \item Could be applied to \textbf{other modalities} if reasonable augmentations are available
        \item Could combine SGF with \textbf{other self-supervised learning methods}
    \end{itemize}
\end{frame}






\end{document}