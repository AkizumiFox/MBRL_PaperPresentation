\documentclass[9pt]{beamer}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{algpseudocode}

% TikZ and related libraries
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

\setbeamertemplate{navigation symbols}{}

% Add page number to the lower right corner
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}

% Customize the page number appearance
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}

\newcommand{\sg}{\text{sg}}

\title{From TD-MPC to BOOM}
\subtitle{Bootstrap Off-policy with World Model (NeurIPS 2025 Poster)}
% \date{November 3, 2025}

\begin{document}

%% Slide 1 — Title & Story
\begin{frame}
    \titlepage
    
    \vspace{1em}
    \textbf{We'll cover:}
    \begin{enumerate}
        \item Why latent MPC at all
        \item TD-MPC(-2): components in one design
        \item Acting: MPPI is the ``planner''
        \item Hidden weakness: behavior = planner, learning = policy
        \item BOOM: value-weighted alignment with planner
    \end{enumerate}
\end{frame}

%% Slide 2 — Motivation: Latent World Model + MPC
\begin{frame}
    \frametitle{Motivation: Latent World Model + MPC}
    
    \begin{itemize}
        \item Want to \textbf{act from pixels / high-dim states} but still plan
        \item Learn a \textbf{compact latent} $z_t$ so planning is cheap
        \item Then: $(z_t, a_t) \rightarrow \hat{z}_{t+1}$, $(z_t, a_t) \rightarrow \hat{r}_t$, $z_t \rightarrow V(z_t)$
        \item At test time: \textbf{plan in latent}, not in pixel space
        \item Always execute \textbf{only the first action} $\rightarrow$ MPC / receding horizon
    \end{itemize}
    
    \vspace{1em}
    \textbf{Key insight:} This family doesn't do ``Dreamer-style: imagine to train a policy, then act.'' It does ``PlaNet/PETS-style: \textbf{use the model at action time}.'' To make that fast and stable, they move everything into a latent and remove decoders.
\end{frame}

%% Slide 3 — Unified TD-MPC(-2) Model
\begin{frame}
    \frametitle{Unified TD-MPC(-2) Model}
    
    We learn \textbf{five} things jointly:
    
    \begin{enumerate}
        \item \textbf{Encoder} $h_\theta$: $z_t = h_\theta(s_t)$
        \item \textbf{Latent dynamics} $f_\theta$: $\hat{z}_{t+1} = f_\theta(z_t, a_t)$
        \item \textbf{Reward head} $R_\theta$: $\hat{r}_t = R_\theta(z_t, a_t)$
        \item \textbf{Value head} $V_\theta$: $\hat{v}_t = V_\theta(z_t)$
        \item \textbf{Policy} $\pi_\theta(z_t)$: to \textbf{seed} the planner, and maybe act fast
    \end{enumerate}
    
    \vspace{1em}
    \textbf{No:} image decoder, ELBO, KL.\\
    \textbf{Yes:} task-only latent that stays consistent.
    
    \vspace{1em}
    \small{This is TD-MPC \textit{and} TD-MPC-2: both are decoder-free, both have dynamics+reward+value in the same latent, both may include a small policy to warm-start the planner. TD-MPC-2 just \textbf{scales} this backbone and makes it multi-task-robust.}
\end{frame}

%% Slide 4 — Training Loss (Short Latent Rollout)
\begin{frame}
    \frametitle{Training Loss (Short Latent Rollout)}
    
    Given replay $(s_t, a_t, r_t, s_{t+1}), \dots$:
    
    \begin{enumerate}
        \item \textbf{Encode reals:} $z_{t+i}^{\text{enc}} = h_\theta(s_{t+i})$
        \item \textbf{Predict next latent:} $\hat{z}_{t+i+1} = f_\theta(z_{t+i}^{\text{enc}}, a_{t+i})$
        \item \textbf{Dynamics consistency:}
        \[
        L_{\text{dyn}} = \sum_i \lVert \hat{z}_{t+i+1} - z_{t+i+1}^{\text{enc}} \rVert^2
        \]
        \item \textbf{Reward prediction:}
        \[
        L_{\text{rew}} = \sum_i (\hat{r}_{t+i} - r_{t+i})^2
        \]
        \item \textbf{TD value:} target $y_{t+i} = r_{t+i} + \gamma V_{\bar{\theta}}(z_{t+i+1}^{\text{enc}})$
        \[
        L_{\text{val}} = \sum_i (V_\theta(z_{t+i}^{\text{enc}}) - \sg(y_{t+i}))^2
        \]
    \end{enumerate}
    
    \textbf{Total:}
    \[
    L = L_{\text{dyn}} + \alpha L_{\text{rew}} + \beta L_{\text{val}}
    \]
\end{frame}

%% Slide 5 — Acting Objective (This Is the MPC Part)
\begin{frame}
    \frametitle{Acting Objective (This Is the MPC Part)}
    
    At real time $t$, with current latent $z_t$, solve:
    
    \[
    \max_{a_{t:t+H-1}}
    \left(
    \sum_{h=0}^{H-1} \gamma^{h} R_\theta(z_{t+h}, a_{t+h})
    +
    \gamma^{H} V_\theta(z_{t+H})
    \right)
    \]
    \[
    \text{s.t.}\quad
    z_{t+h+1} = f_\theta(z_{t+h}, a_{t+h})
    \]
    
    \vspace{1em}
    \begin{itemize}
        \item \textbf{Short horizon} $H$ (e.g. 5--10) is OK
        \item Because we \textbf{bootstrap} with $V_\theta$ at the end
    \end{itemize}
    
    \vspace{1em}
    \textbf{This is why it's ``TD-MPC'':} it's MPC in latent, but the tail is \textbf{closed by a TD value}. That's the difference from PETS/PlaNet, which had to plan longer.
\end{frame}

%% Slide 6 — MPPI (the Planner)
\begin{frame}
    \frametitle{MPPI (the Planner)}
    
    MPPI = \textbf{sampling-based optimizer}, not a NN:
    
    \begin{enumerate}
        \item Keep a mean action sequence $\{\mu_h\}_{h=0}^{H-1}$ (often initialized from $\pi_\theta(z_t)$)
        \item Sample $N$ noisy sequences around it
        \item Roll each in latent with $f_\theta, R_\theta, V_\theta$
        \item Score each sequence (higher return $\rightarrow$ lower cost)
        \item Turn scores into \textbf{weights} (softmax with temperature)
        \item Weighted-average back to a better mean
        \item Execute \textbf{only first} action
    \end{enumerate}
    
    \vspace{1em}
    \textbf{Key point:} MPPI is \textbf{just an algorithm} that calls the learned model many times. It's where the ``planner'' actually is. TD-MPC-2 runs this \textbf{every env step}. That's why the real behavior comes from the planner.
\end{frame}

%% Slide 7 — Behavior Policy (β): What Really Acts
\begin{frame}
    \frametitle{Behavior Policy ($\beta$): What Really Acts}
    
    \begin{itemize}
        \item Replay stores $(s_t, a_t, r_t, s_{t+1})$
        \item But $a_t$ came from \textbf{planner-augmented control}:
        \[
        a_t = \text{MPPI}\big(z_t, f_\theta, R_\theta, V_\theta, \text{seed}=\pi_\theta(z_t)\big)
        \]
        \item So define \textbf{behavior policy}:
        \[
        \beta(a \mid s) = \text{distribution induced by MPPI at } s
        \]
        \item \textbf{Key point:} $\beta \neq \pi_\theta$
    \end{itemize}
    
    \vspace{1em}
    \textbf{Subtlety:} we \textit{think} we have a policy network, but the data we actually learn from was produced by \textbf{planner + model + maybe policy as seed}. So the true behavior is $\beta$, not $\pi$. Keep that in mind --- BOOM will grab this.
\end{frame}

%% Slide 8 — Where TD-MPC(-2) Starts to Crack
\begin{frame}
    \frametitle{Where TD-MPC(-2) Starts to Crack}
    
    \begin{itemize}
        \item Critic and model are trained on \textbf{$\beta$-data} (planner's actions)
        \item But policy update is on \textbf{$\pi$} (``pick high-$Q$'')
        \item If $\pi$ can't represent planner moves $\rightarrow$ $\pi$ drifts
        \item Then:
        \begin{itemize}
            \item critic sees actions $\pi$ never executes
            \item policy sees critic trained off its distribution
            \item learning becomes noisy / unstable
        \end{itemize}
    \end{itemize}
    
    \vspace{1em}
    \textbf{This is called \textit{planner--policy divergence} or \textit{actor divergence}.} It's specific to ``plan every step'' world-model RL. Dreamer doesn't hit it because Dreamer \textit{acts with the policy}, not with a separate planner.
\end{frame}

%% Slide 9 — BOOM: Goal
\begin{frame}
    \frametitle{BOOM: Goal}
    
    \begin{itemize}
        \item \textbf{Keep using the strong planner} (MPPI) to collect good data
        \item \textbf{Make the policy chase the planner} so $\pi \approx \beta$
        \item Do it \textbf{without} needing a density for the planner
        \item Prefer to imitate \textbf{good} planner actions, not all of them
    \end{itemize}
    
    \vspace{2em}
    \textbf{Key insight:} BOOM is not replacing TD-MPC-2. It's \textbf{wrapped around it}: ``you have planner-collected data; here is how to make your NN policy stay on that distribution.''
\end{frame}

%% Slide 10 — BOOM: Forward-KL / BC on Planner
\begin{frame}
    \frametitle{BOOM: Forward-KL / BC on Planner}
    
    We \textbf{can't} do $\mathrm{KL}(\pi \parallel \beta)$ (planner is likelihood-free).
    
    So do
    \[
    \mathrm{KL}(\beta \parallel \pi)
    = \mathbb{E}_{a \sim \beta} \big[ - \log \pi(a \mid s) \big] + \text{const}.
    \]
    
    So just add
    \[
    L_{\text{align}} = \mathbb{E}_{(s,a)\sim \text{replay}} \big[ - \log \pi(a \mid s) \big]
    \]
    
    = \textbf{behavior cloning from planner actions} in the buffer.
    
    \vspace{1em}
    \textbf{This is the key BOOM trick:} since the planner is the thing that \textbf{actually} produced the dataset, we can just \textbf{imitate its actions}. That directly reduces the $\pi$ vs $\beta$ gap.
\end{frame}

%% Slide 11 — BOOM: Value-Weighted Alignment
\begin{frame}
    \frametitle{BOOM: Value-Weighted Alignment}
    
    Not all planner actions are good $\rightarrow$ weight them:
    
    \begin{enumerate}
        \item For batch $(s_i, a_i)$, get $Q(s_i, a_i)$
        \item Weights:
        \[
        w_i = \frac{\exp(Q(s_i, a_i)/\tau)}{\sum_j \exp(Q(s_j, a_j)/\tau)}
        \]
        \item Aligned loss:
        \[
        L_{\text{align}} = \sum_i w_i \big( - \log \pi(a_i \mid s_i) \big)
        \]
        \item Final policy loss:
        \[
        L_{\text{policy}} = - \mathbb{E}_s Q(s, \pi(s)) + \lambda L_{\text{align}}
        \]
    \end{enumerate}
    
    \vspace{1em}
    So the policy is pushed in \textbf{two} directions: (1) classic RL: pick high-$Q$ actions, (2) alignment: don't leave the planner's support, especially where the planner was strong. This is exactly the failure mode we saw in unified TD-MPC/TD-MPC-2.
\end{frame}

%% Slide 12 — Recap
\begin{frame}
    \frametitle{Recap}
    
    \begin{itemize}
        \item \textbf{TD-MPC / TD-MPC-2} = decoder-free latent world model + reward + value + \textbf{online MPPI} every step
        \item Real behavior = \textbf{planner-augmented policy} $\beta$, not $\pi$
        \item That causes \textbf{planner--policy divergence}
        \item \textbf{BOOM}: add value-weighted behavior cloning from planner actions $\rightarrow$ $\pi \to \beta$
        \item Result: policy, critic, model, planner all stay on \textbf{one} visitation distribution
    \end{itemize}
    
    \vspace{2em}
    \textbf{Summary:} We didn't split TD-MPC and TD-MPC-2; we treated them as one design line: ``latent MPC with TD.'' That design is powerful but naturally creates a two-actor world (planner vs policy). BOOM is the current clean fix for that exact pattern.
\end{frame}

\end{document}