\documentclass[9pt]{beamer}
\input{commands.tex}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{algpseudocode}
\usepackage{sidecap}

% TikZ and related libraries
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
% \usepackage{siunitx}
\usepackage{makecell}

\usepackage[sort]{natbib}

% \input{math_commands.tex}

\AtBeginDocument{\RenewCommandCopy\qty\SI}

\pgfplotsset{compat=newest}

\setbeamertemplate{navigation symbols}{}

% Add page number to the lower right corner
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}

% Customize the page number appearance
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}

\title{Curious Replay for Model-based Adaptation}
\setbeamerfont{title}{size=\huge}
\date{July 2, 2025}

\begin{document}

\begin{frame}
    \titlepage % This command creates the title page
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay for Model-based Adaptation (\citep{kauvar2023curiousreplaymodelbasedadaptation})}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay Algorithm}
    
    \begin{algorithmic}[1]
        \State {\bfseries Input:} Replay buffer $R$ that uses a SumTree structure to store the priority $p_i$ of each transition
        \State {\bfseries Hyperparameters:} $c$, $\beta$, $\alpha$, $\epsilon$, environment steps per train step $L$, batch size $B$, maximum priority $p_{\text{MAX}}$
        \For{iteration 1, 2, \ldots }
            \State Collect $L$ transitions $(x_t, a_t, r_t, x_{t+1})$ with policy
            \State Add transitions to replay buffer $R$, each with priority $p_i \leftarrow p_{\text{MAX}}$ and visit count $v_i \leftarrow 0$
            \State Sample batch of $B$ transitions from $R$ using probability for selecting transition $i$ as $p_i / \sum_{j = 1}^{|R|}{p_j}$
            \State Train world model and policy using batch, and cache loss $\mathcal{L}_{i}$ for each transition in batch
            \For{transition $i$ in batch}
               \State $p_i \leftarrow c\beta^{v_{i}} + (|\mathcal{L}_{i}| + \epsilon)^\alpha$
               \State $v_{i} \leftarrow v_{i}+1$
            \EndFor
        \EndFor
   \end{algorithmic}   
    
\end{frame}

\begin{frame}
    \frametitle{Curious Replay: Motivation}

    \begin{itemize}
        \item \textbf{Challenge:} After an environment change, the world model is suddenly inaccurate on new data.
        \item \textbf{Goal:} Adapt quickly by focusing learning on the most relevant transitions.
        \item \textbf{Solution:} \textbf{Curious Replay} prioritizes replay buffer sampling to:
        \begin{itemize}
            \item Fix transitions the model predicts poorly \textit{right now}
            \item Ensure new, rarely trained transitions are not neglected
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay: The Two Key Signals}

    \begin{enumerate}
        \item \textbf{Model Error:} Use the world modelâ€™s prediction loss $\mathcal{L}_i$ for each transition $i$.
        \begin{itemize}
            \item Higher $|\mathcal{L}_i|$ means the model is more wrong $\rightarrow$ sample it more.
        \end{itemize}
        \item \textbf{Under-Replay:} Track a visit count $v_i$ for each transition.
        \begin{itemize}
            \item Fewer replays (lower $v_i$) $\rightarrow$ boost its priority to ensure coverage, especially for new data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay: Priority Formula}

    \textbf{Priority for transition $i$:}
    \begin{equation*}
        p_i \leftarrow c\,\beta^{v_i} + (|\mathcal{L}_i| + \epsilon)^{\alpha}
    \end{equation*}
    \begin{itemize}
        \item $c > 0$: base curiosity bonus for rarely trained items
        \item $0 < \beta < 1$: decays the curiosity bonus as $i$ is replayed more
        \item $\alpha \in (0,1]$: softens extremes (like in PER)
        \item $\epsilon > 0$: avoids zero priority
    \end{itemize}
    \vspace{1em}
    \textbf{Sampling probability:}
    \begin{equation*}
        P(i) = \frac{p_i}{\sum_j p_j}
    \end{equation*}
    \vspace{1em}
    \textbf{After sampling \( i \)-th transition:}
    \[
        v_i \leftarrow v_i + 1
    \]
\end{frame}

\begin{frame}
    \frametitle{Curious Replay: Why It Works}

    \begin{itemize}
        \item \textbf{After a change:}
        \begin{itemize}
            \item New/shifted transitions have high $|\mathcal{L}_i|$ and low $v_i$ $\rightarrow$ get sampled more.
        \end{itemize}
        \item \textbf{As learning progresses:}
        \begin{itemize}
            \item Both error and curiosity bonus decrease, so focus shifts to other transitions.
        \end{itemize}
        \item \textbf{Result:} The world model adapts quickly, and the policy can trust its imagined rollouts again.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay vs. Classic PER}

    \begin{itemize}
        \item \textbf{PER:} Prioritizes by value TD error (how surprising the reward is).
        \item \textbf{Curious Replay:} Prioritizes by world-model error (how surprising the transition is) \textbf{and} a freshness bonus.
        \item \textbf{Why?} In Dreamer, model accuracy is the bottleneck for adaptation after distribution shifts.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Curious Replay: Practical Tips}

    \begin{itemize}
        \item Start with $\alpha \in [0.5,1]$, $\beta \in [0.8,0.99]$, small $\epsilon$
        \item Choose $c$ so new items get sampled promptly but don't dominate
        \item Cap priorities at $p_{\text{MAX}}$ to avoid outliers
        \item (Optional) Add importance-sampling weights for strict bias control, but the simple scheme works well in practice
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment: Interaction with Object Introduced Halfway}
    
    \begin{figure}
        \centering
        \begin{minipage}{0.6\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/image.png}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.35\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/image2.png}
        \end{minipage}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiment: Constrained Control Released Halfway}

    \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{figures/image3.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiment: CR Helps for Crafter}

    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{figures/image4.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiment: Background Swap in DMC}

    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/image5.png}
    \end{figure}
    
\end{frame}


\end{document}