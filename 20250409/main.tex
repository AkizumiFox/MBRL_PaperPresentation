\documentclass[9pt]{beamer}
\input{commands}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage[sort]{natbib}
\input{macros.sty}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\AtBeginDocument{\RenewCommandCopy\qty\SI}
\pgfplotsset{compat=newest}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}
\setbeamerfont{title}{size=\huge}


\title{Improving Token-Based World Models\\with Parallel Observation Prediction}
\date{April 09, 2025}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\begin{frame}
    \frametitle{Overview}
    
    \begin{itemize}
        \item REM \citep{cohen2024improvingtokenbasedworldmodels} builds on IRIS, following a $\Tokenizer$-$\WM$-$\Controller$ structure:
        \begin{itemize}
            \item $\Tokenizer$: Visual perception module compressing observations
            \item $\WM$: Predictive model capturing environment dynamics
            \item $\Controller$: Controller learning to maximize return
        \end{itemize}
        \item Uses replay buffer to store environment interaction data
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$\Tokenizer$ - Tokenizer}
    
    \begin{itemize}
        \item Implemented as a VQ-VAE discrete auto-encoder
        \item Components:
        \begin{itemize}
            \item Encoder: Maps input image $\obs_t$ to latent vectors $(\tknzrLatent^1_t,\tknzrLatent^2_t,\cdots,\tknzrLatent^K_t)$
            \item Embedding table: $\tknzrCodebook=\{\tknEmb_i\}_{i=1}^N\in\mathbb{R}^{N\times d}$ with $N$ trainable vectors
            \item Tokens: $\token_{t}^{k} = \argmin_i \Vert \tknzrLatent_{t}^{k} - \tknEmb_{i} \Vert$
            \item Decoder: Maps token embeddings back to reconstructed observation $\hat{\obs}_t$
        \end{itemize}
        \item Trained on frames sampled uniformly from replay buffer
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$\WM$ - World Model}
    
    \begin{itemize}
        \item Models environment dynamics in latent token space
        \item Predicts three distributions at each step $t$:
        \begin{itemize}
            \item Transition: $p(\hat{\tokens}_{t+1} \vert \tokens_{1}, \action_{1}, \ldots, \tokens_{t}, \action_{t})$
            \item Reward: $p(\hat{\reward}_{t} \vert \tokens_{1}, \action_{1}, \ldots, \tokens_{t}, \action_{t})$
            \item Termination: $p(\hat{\doneSgnl}_{t} \vert \tokens_{1}, \action_{1}, \ldots, \tokens_{t}, \action_{t})$
        \end{itemize}
        \item Uses code vectors $\tknzrCodebook$ learned by tokenizer (not updated by $\WM$)
        \item Maintains dedicated embedding tables for actions and special tokens
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$\Controller$ - Controller \& Chunkwise Computation}
    
    \begin{itemize}
        \item Actor-critic controller trained in imagination
        \item Components:
        \begin{itemize}
            \item Policy network $\policy$
            \item Value function estimator $V^{\policy}$
        \end{itemize}
        \item Training process:
        \begin{itemize}
            \item Initialize with trajectory from replay buffer
            \item Interact with world model for $\horizon$ steps
            \item Sample actions from policy
            \item World model generates rewards, terminations, and next tokens
            \item Use trajectories to train agent
        \end{itemize}
        \item Computational efficiency:
        \begin{itemize}
            \item "Chunkwise" computation splits sequences into smaller chunks
            \item Previous chunks summarized by recurrent state $\retnetState$
            \item Enhanced by POP extension for efficient training
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Retention Architecture}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig3}
        \caption{Retention mechanism with chunkwise computation.}
        \label{fig:retention}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Retention Formulation}
    
    \begin{itemize}
        \item For token sequence $(x_1, x_2, \cdots, x_m)$ with embeddings $\mathbf{X}\in\mathbb{R}^{m\times d}$
        \item In RL context: sequence is token trajectory of observation-action blocks
        \item Split into chunks of $\retnetChunkSize$ tokens where $\retnetChunkSize = \blocksPerChunk (\tokensPerObs + 1)$
        \item Retention output for $i$-th chunk:
        \begin{align*}
            \retnetY_{[i]} = \left( \retnetQ_{[i]} \retnetK^{\Tr}_{[i]} \odot \retnetD \right) \retnetV_{[i]} + (\retnetQ_{[i]} \retnetState_{[i-1]}) \odot \boldsymbol{\xi}
        \end{align*}
        \item Recurrent state update:
        \begin{align*}
            \retnetState_{[i]} = (\retnetK_{[i]} \odot \boldsymbol{\zeta})^{\Tr} \retnetV_{[i]} + \retnetEta^{\retnetChunkSize} \retnetState_{[i-1]}
        \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Retention Notation}
    
    \begin{itemize}
        \item RetNet model consists of stacked layers with Retention mechanism
        \item Retention has dual form of recurrence and parallelism ("chunkwise")
        \begin{itemize}
            \item Splits long sequences into smaller chunks
            \item Parallel computation within chunks
            \item Sequential recurrent form between chunks
        \end{itemize}
        \item Information from previous chunks summarized by recurrent state $\retnetState\in\mathbb{R}^{d\times d}$
        \item Retention mechanism formulation:
        \begin{itemize}
            \item where the bracketed subscript $[i]$ is used to index the $i$-th chunk
            \item $\retnetQ = \left( \retnetInput \retnetW_{Q} \right) \odot \retnetPos$
            \item $\retnetK =  \left( \retnetInput \retnetW_{K} \right) \odot \retnetPosC$
            \item $\retnetV = \retnetInput \retnetW_{V}$
            \item $\boldsymbol{\xi}\in\mathbb{R}^{B\times d}$ is a matrix with $\boldsymbol{\xi}_{ij} = \retnetEta^{i+1}$
            \item $\retnetW_{Q},\retnetW_{K},\retnetW_{V}\in\mathbb{R}^{d\times d}$ are learnable weights
            \item $\retnetEta$ is an exponential decay factor
            \item the matrix $\retnetD\in\mathbb{R}^{\retnetChunkSize \times \retnetChunkSize}$ combines an auto-regressive mask with the temporal decay factor $\retnetEta$
            \item the matrices $\retnetPos,\retnetPosC\in\mathbb{C}^{m\times d}$ are for relative position embedding
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model Imagination (Figure)}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig4}
        \caption{World model imagination.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{World Model Imagination (1/2)}
    
    \begin{itemize}
        \item Agent training relies entirely on world model imagination
        \item For trajectory generation, we need to predict:
        \begin{itemize}
            \item Reward $\hat{\reward}_t$
            \item Termination signal $\hat{\doneSgnl}_t$
            \item $K$ latent observation tokens $\hat{\tokens}_{t+1}$
        \end{itemize}
        \item Predicting $\hat{\tokens}_{t+1}$ is the computational bottleneck
        \item In IRIS, token prediction is sequential:
        \begin{itemize}
            \item Each token predicted one at a time
            \item Requires $\tokensPerObs \horizon$ sequential world model calls
            \item Poor GPU utilization and long computation time
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model Imagination (2/2)}
    
    \begin{itemize}
        \item POP's solution: dedicated prediction tokens
        \item Maintains set of $\tokensPerObs$ prediction tokens $\predTokens = (\predToken_{1},\ldots,\predToken_{\tokensPerObs})$
        \item To generate $\hat{\tokens}_{t+1}$ in one pass:
        \begin{itemize}
            \item Compute RetNet outputs from recurrent state $\retnetState_{t}$
            \item Use $\predTokens$ as input sequence
            \item Chunk size limited to single block ($\tokensPerObs + 1$)
        \end{itemize}
        \item Benefits:
        \begin{itemize}
            \item Reduces world model calls from $\tokensPerObs \horizon$ to $2\horizon$
            \item Eliminates dependency on number of tokens $\tokensPerObs$
            \item Improves scalability at expense of higher overall computation
        \end{itemize}
        \item Follows trend of favoring scalability over raw computational cost
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model Training (Figure)}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig6}
        \caption{An illustration of the POP chunkwise forward algorithm (Alg. 1 and 2) for a single-layer model. During training, $\WM$ computes the outputs of $\blocksPerChunk$ observation-action blocks in parallel. Blue squares represent token inputs, while the corresponding RetNet outputs are denoted by circles.
        Each RetNet block represents a forward call to the same RetNet model. 
        The bottom RetNet call uses our POP extension for computing the additional recurrent states at the end of every observation-action block (Alg. 2, lines 2-7). 
        The top row of RetNet calls are batch-computed in parallel (Alg. 2, line 8). 
        Finally, the output combines the observation token outputs produced by the top RetNet call with the rewards and termination outputs computed by the bottom one (Alg. 1, lines 7-9).}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{World Model Training (1/3)}
    
    \begin{itemize}
        \item Challenge: POP requires special training data preparation
        \item For each time step $t$, model must predict $\tokens_{t}$ given:
        \begin{itemize}
            \item Trajectory prefix $(\tokens_{1}, \action_{1}, \ldots, \tokens_{t-1}, \action_{t-1})$
            \item Prediction tokens $\predTokens$
        \end{itemize}
        \item Problem: Cannot simply replace tokens with prediction tokens
        \begin{itemize}
            \item Original tokens needed for future predictions
            \item Standard approach not viable - requirements contradict
        \end{itemize}
        \item Need efficient method to compute outputs for all time steps in parallel
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model Training (2/3)}
    
    \begin{itemize}
        \item Solution: Summarize trajectory prefixes into recurrent states
        \item For chunk $(\tokens_1, \action_1, \ldots, \tokens_{\blocksPerChunk}, \action_{\blocksPerChunk})$:
        \begin{itemize}
            \item $(\tokens_1, a_1)$ summarized into $\retnetState_{[1, 1]}$
            \item $(\tokens_1, a_1, \tokens_2, a_2)$ summarized into $\retnetState_{[1, 2]}$
        \end{itemize}
        \item Two-step computation to get all recurrent states $\retnetState_{[i, j]}$:
        \begin{itemize}
            \item Compute intermediate states $\Tilde{\retnetState}_{[i, j]}$ in parallel:
            \begin{align*}
                \Tilde{\retnetState}_{[i, j]} = \left( \retnetK_{[i, j]} \odot \boldsymbol{\zeta} \right)^{\Tr} \retnetV_{[i, j]}
            \end{align*}
            \item Compute final states sequentially with minimal overhead:
            \begin{align*}
                \retnetState_{[i, j]} = \Tilde{\retnetState}_{[i, j]} + \retnetEta^{\tokensPerObs + 1} \retnetState_{[i, j-1]}
            \end{align*}
        \end{itemize}
        \item Then predict all next observations from $(\retnetState_{[i, j]}, \predTokens)$ tuples
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Model Training (3/3)}
    
    \begin{itemize}
        \item Training process:
        \begin{itemize}
            \item Sample trajectory segments of $\horizon$ steps from replay buffer
            \item Process in chunks of $\blocksPerChunk$ observation-action blocks
            \item Produce modeled distributions using POP chunkwise forward
        \end{itemize}
        \item Optimization:
        \begin{itemize}
            \item Minimize cross-entropy loss for transitions and termination
            \item For rewards: MSE loss (continuous) or cross-entropy (discrete)
        \end{itemize}
        \item Key innovation: Extended RetNet to support batched computation with appropriate positional encoding
        \item Algorithm illustrated in Figure:
        \begin{itemize}
            \item Bottom RetNet call computes additional recurrent states
            \item Top row of RetNet calls batch-computed in parallel
            \item Output combines observation token outputs with rewards and termination
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Algorithm}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/alg1}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Algorithm}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/alg2}
    \end{figure}
\end{frame}


\end{document}