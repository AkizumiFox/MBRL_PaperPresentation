\documentclass[9pt]{beamer}
\input{commands}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{sidecap}

% TikZ and related libraries
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{arrows,arrows.meta,calc}
\usetikzlibrary{patterns,backgrounds}
\usetikzlibrary{positioning,fit}
\usetikzlibrary{shapes.geometric,shapes.multipart}
\usetikzlibrary{patterns.meta,decorations.pathreplacing,calligraphy}
\usetikzlibrary{tikzmark}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
% \usepackage{siunitx}
\usepackage{makecell}

\usepackage[sort]{natbib}

\AtBeginDocument{\RenewCommandCopy\qty\SI}

\pgfplotsset{compat=newest}

\newcommand{\blap}[1]{\vbox to 0pt{\hbox{#1}\vss}}

\newcommand{\enc}{h}
\newcommand{\Infop}{\info_{p,h}}
\newcommand{\Expp}{\Exp_{p,h}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\scoref}{\nabla_\x \log p^\tau(\x)}
\newcommand{\scorem}{\mathbf{S}_\theta(\x, \tau)}
\newcommand{\bbe}{\mathbb{E}}
\newcommand{\Tau}{\mathcal{T}}

\setbeamertemplate{navigation symbols}{}

% Add page number to the lower right corner
\setbeamertemplate{footline}{%
  \hfill\usebeamercolor[fg]{page number in head/foot}%
  \usebeamerfont{page number in head/foot}%
  \insertframenumber\,/\,\inserttotalframenumber\hspace*{1ex}\vskip2pt%
}

% Customize the page number appearance
\setbeamercolor{page number in head/foot}{fg=gray}
\setbeamerfont{page number in head/foot}{size=\small}

\title{Transformers are Sample-Efficient\\World Models}
\setbeamerfont{title}{size=\huge}
\date{March 5, 2025}

% Add the new commands
\newcommand{\repolink}{\href{https://github.com/eloialonso/diamond}{\texttt{https://github.com/eloialonso/diamond}}}
\newcommand{\wslink}{\href{https://diamond-wm.github.io}{\texttt{https://diamond-wm.github.io}}}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Reference}

    \bibliographystyle{plainnat}
    \bibliography{references}
\end{frame}

\begin{frame}
    \frametitle{Sample Efficiency Challenge in World Models}
    
    \begin{itemize}
        \item Common drawback: Extremely low sample efficiency
        \item Experience requirements:
        \begin{itemize}
            \item DreamerV2: Months of gameplay for Atari 2600
            \item OpenAI Five: Thousands of years for Dota2
        \end{itemize}
        \item Real-world limitations:
        \begin{itemize}
            \item Cannot always speed up training environments
            \item Cost and safety considerations
        \end{itemize}
        \item \textbf{Key insight:} Sample efficiency is crucial for practical deployment of deep RL agents
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{World Models: Progress and Challenges}
    
    \begin{itemize}
        \item Model-based methods offer promising path to data efficiency
        \item Recent applications of world models:
        \begin{itemize}
            \item Pure representation learning
            \item Lookahead search
            \item Learning in imagination
        \end{itemize}
        \item Learning in imagination advantages:
        \begin{itemize}
            \item Frees agent from sample efficiency constraints
            \item But requires highly accurate world models
        \end{itemize}
        \item Key milestones:
        \begin{itemize}
            \item Early success in toy environments
            \item SimPLe: Progress on Atari 100k benchmark
            \item DreamerV2: Current best but requires 200M frames
        \end{itemize}
        \item \textbf{Challenge:} Need better architectures for complex environments with limited samples
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Transformers: A Promising Direction for World Models}
    
    \begin{itemize}
        \item Transformers have shown remarkable success across domains:
        \begin{itemize}
            \item Natural Language Processing
            \item Computer Vision
            \item Offline Reinforcement Learning
        \end{itemize}
        \item Key advantages:
        \begin{itemize}
            \item Effective with high-dimensional data
            \item Self-supervised learning capabilities
            \item Strong performance with discrete tokens
        \end{itemize}
        \item Addressing computational challenges:
        \begin{itemize}
            \item Direct pixel-to-token conversion impractical
            \item Solution: Discrete autoencoders (VQGAN, DALL-E)
            \item Maps raw pixels to manageable token sets
        \end{itemize}
        \item \textbf{Opportunity:} Transformer-based architectures show promise for efficient world modeling
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{IRIS: Transformer-Based World Model for Sample-Efficient RL}
    
    \begin{itemize}
        \item IRIS (Imagination with auto-Regression over an Inner Speech) \cite{micheli2023transformerssampleefficientworldmodels}:
        \begin{itemize}
            \item Combines discrete autoencoder with autoregressive Transformer
            \item Learns through simulated trajectories in imagination
            \item Treats dynamics learning as sequence modeling problem
        \end{itemize}
        \item Key innovations:
        \begin{itemize}
            \item Autoencoder creates "language" of image tokens
            \item Transformer composes tokens temporally
            \item Minimal hyperparameter tuning needed
        \end{itemize}
        \item Impressive results on Atari 100k benchmark:
        \begin{itemize}
            \item 1.046 mean human-normalized score
            \item Superhuman on 10/26 games
            \item Only 2 hours of real experience needed
            \item Outperforms recent sample-efficient methods
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Learning in Imagination: Three Components}
    
    \begin{itemize}
        \item Our approach follows three standard components for imagination-based learning:
        \begin{itemize}
            \item \texttt{collect\_experience} from environment
            \item \texttt{update\_world\_model} from collected data
            \item \texttt{update\_behavior} within learned model
        \end{itemize}
        \item Key characteristics:
        \begin{itemize}
            \item Agent learns to act exclusively in world model
            \item Real experience only used for dynamics learning
            \item Follows successful approaches like Dreamer V1/V2
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mathematical Breakdown: Encoding}

    Given an input image:
    \[
    x_t \in \mathbb{R}^{h \times w \times 3},
    \]

    The encoder \( E \) (CNN) maps it to continuous latent vectors:
    \[
    y_t = \{ y_t^k \}_{k=1}^K \subset \mathbb{R}^d,
    \]

    Formally:
    \[
    E(x_t) = y_t \in \mathbb{R}^{K \times d}.
    \]
\end{frame}

\begin{frame}
    \frametitle{Mathematical Breakdown: Quantization to Discrete Tokens}

    Define embedding table (codebook):
    \[
    \mathcal{E} = \{ e_i \}_{i=1}^N \subset \mathbb{R}^d.
    \]

    For each latent vector \( y_t^k \), select closest embedding vector:
    \[
    z_t^k = \arg\min_{i \in \{1,\dots,N\}} \| y_t^k - e_i \|_2,
    \]

    Resulting discrete token sequence:
    \[
    z_t = (z_t^1, z_t^2, \dots, z_t^K) \in \{1, \dots, N\}^K.
    \]
\end{frame}

\begin{frame}
    \frametitle{Mathematical Breakdown: Decoding}

    Decoder \( D \) maps token sequence back to image space:
    \[
    \hat{x}_t = D(z_t),
    \]

    Formally:
    \[
    D : \{1, \dots, N\}^K \rightarrow \mathbb{R}^{h \times w \times 3}.
    \]
\end{frame}

\begin{frame}
    \frametitle{Mathematical Breakdown: Loss Functions and Training}

    Let's break down the loss function into its four components:

    \begin{enumerate}
        \item \textbf{Reconstruction Loss}:  
        Measures the pixel-level difference between input \(x\) and reconstruction \(D(z)\).
        \[
        \| x - D(z) \|_{1}
        \]

        \item \textbf{Commitment Loss Terms}:  
        Align encoder output \(E(x)\) with embedding \(\mathcal{E}(z)\). The stop-gradient operator \(\mathrm{sg}(\cdot)\) decouples encoder and embedding updates, facilitating discrete latent space learning.
        \[
        \| \mathrm{sg}(E(x)) - \mathcal{E}(z) \|_{2}^{2} + \| \mathrm{sg}(\mathcal{E}(z)) - E(x) \|_{2}^{2}
        \]

        \item \textbf{Perceptual Loss}:  
        Compares high-level features using a pre-trained network (e.g., VGG), ensuring semantic and perceptual similarity.
        \[
        \mathcal{L}_{\text{perceptual}}(x, D(z))
        \]
    \end{enumerate}

    The total loss function is given by:
    \begin{equation*}
        \mathcal{L}(E, D, \mathcal{E}) = \| x - D(z) \|_{1} + \| \mathrm{sg}(E(x)) - \mathcal{E}(z) \|_{2}^{2} + \| \mathrm{sg}(\mathcal{E}(z)) - E(x) \|_{2}^{2} + \mathcal{L}_{\text{perceptual}}(x, D(z))
    \end{equation*}

    This combination encourages accurate reconstruction, consistent discrete embeddings, and high perceptual quality.
\end{frame}

\begin{frame}
    \frametitle{Mathematical Breakdown: Straight-Through Estimator}

    Quantization step is non-differentiable:
    \[
    z_t^k = \arg\min_{i} \| y_t^k - e_i \|_2
    \]

    Straight-through estimator enables gradient flow:
    \begin{itemize}
        \item Copies gradients from decoder \(D\) back to encoder \(E\)
        \item Treats quantization as identity during backward pass
    \end{itemize}

    Allows end-to-end training despite discrete quantization.
\end{frame}

\begin{frame}
    \frametitle{Transformer-Based Sequence Modeling}

    The Transformer \( G \) learns environment dynamics by modeling sequences of encoded observations and actions as a "language":

    \textbf{Sequence Representation:}
    \[
    (x_0, a_0, x_1, a_1, \dots, x_t, a_t)
    \]

    Each image \( x_t \) is encoded into discrete tokens:
    \[
    (z_t^1, z_t^2, \dots, z_t^K)
    \]

    Resulting interleaved sequence:
    \[
    (z_0^1, \dots, z_0^K, a_0, z_1^1, \dots, z_1^K, a_1, \dots, z_t^1, \dots, z_t^K, a_t)
    \]
\end{frame}

\begin{frame}
    \frametitle{Predicted Distributions}

    At each timestep, Transformer predicts three key elements:

    \begin{itemize}
        \item \textbf{Transition (next state tokens):}
        \[
        p_G\big(\hat{z}_{t+1} \mid z_{\le t}, a_{\le t}\big)
        \]

        \item \textbf{Reward:}
        \[
        p_G\big(\hat{r}_t \mid z_{\le t}, a_{\le t}\big)
        \]

        \item \textbf{Termination (episode end):}
        \[
        p_G\big(\hat{d}_t \mid z_{\le t}, a_{\le t}\big)
        \]
    \end{itemize}

    Enables complete modeling of environment dynamics.
\end{frame}

\begin{frame}
    \frametitle{Autoregressive Prediction with Transformer \( G \)}

    This is how \(p_G\big(\hat{z}_{t+1} \mid z_{\le t}, a_{\le t}\big)\) works:

    Transformer \( G \) operates at the token level, predicting future states autoregressively:

    To predict next frame tokens \( \hat{z}_{t+1} \):
    \[
    p_G\big(\hat{z}_{t+1}^k \mid z_{\le t}, a_{\le t}, z_{t+1}^{<k}\big)
    \]

    Each individual token prediction conditioned on:
    \begin{itemize}
        \item All previous tokens and actions \( (z_{\le t}, a_{\le t}) \)
        \item Previously predicted tokens of current frame \( z_{t+1}^{<k} \)
    \end{itemize}

    This token-level prediction allows the Transformer to learn fine-grained temporal dependencies and dynamics.
\end{frame}

\begin{frame}
    \frametitle{Training the Transformer}

    Transformer \( G \) trained via self-supervised learning on segments of length \( L \):

    \begin{itemize}
        \item \textbf{Self-Supervised Learning:}
        \begin{itemize}
            \item Predicts future parts of sequences from past experiences
            \item No labeled data required
        \end{itemize}

        \item \textbf{Loss Functions:}
        \begin{itemize}
            \item \textbf{Transition:} Cross-entropy loss (autoregressive token prediction)
            \[
            \mathcal{L}_{\text{trans}} = \sum_{t}\sum_{k=1}^{K}\text{CE}\Big(p_G(\hat{z}_{t+1}^k|z_{\le t},a_{\le t},z_{t+1}^{<k}), z_{t+1}^k\Big)
            \]

            \item \textbf{Termination:} Cross-entropy loss (episode end prediction)
            \[
            \mathcal{L}_{\text{term}} = \sum_{t}\text{CE}\Big(p_G(\hat{d}_t|z_{\le t},a_{\le t}), d_t\Big)
            \]

            \item \textbf{Reward:} MSE (continuous) or cross-entropy (discrete)
            \[
            \mathcal{L}_{\text{reward}} = 
            \begin{cases}
                \text{MSE}(p_G(\hat{r}_t|z_{\le t},a_{\le t}), r_t), & \text{continuous}\\[6pt]
                \text{CE}(p_G(\hat{r}_t|z_{\le t},a_{\le t}), r_t), & \text{discrete}
            \end{cases}
            \]
        \end{itemize}
    \end{itemize}

    Minimizing these losses ensures accurate modeling of transitions, rewards, and termination.
\end{frame}

\begin{frame}
    \frametitle{Actor-Critic Learning Objectives}

    We follow Dreamer V2 for actor-critic training, using the generic \(\lambda\)-return to balance bias and variance:

    Given imagined trajectory \((\hat{x}_0, a_0, \hat{r}_0, \hat{d}_0, \dots, \hat{x}_H)\), define recursively:
    \[
    \Lambda_t = 
    \begin{cases}
        \hat{r}_t + \gamma (1 - \hat{d}_t) \Big[ (1 - \lambda) V(\hat{x}_{t+1}) + \lambda \Lambda_{t+1} \Big], & t < H \\[6pt]
        V(\hat{x}_H), & t = H
    \end{cases}
    \]

    Value network \(V\) minimizes squared difference with \(\lambda\)-returns:
    \[
    \mathcal{L}_V = \mathbb{E}_\pi \Big[ \sum_{t=0}^{H-1} \big( V(\hat{x}_t) - \mathrm{sg}(\Lambda_t) \big)^2 \Big]
    \]

    Actor policy \(\pi\) is trained to maximize expected returns with entropy regularization:
    \[
    \mathcal{L}_\pi = - \mathbb{E}_\pi \Big[ \sum_{t=0}^{H-1} \log(\pi(a_t | \hat{x}_{\le t})) \operatorname{sg}(\Lambda_t - V(\hat{x}_t) ) + \eta \operatorname{\mathcal{H}}(\pi (a_t | \hat{x}_{\le t}))\Big]
    \]

    \textbf{Note:} Actor-critic objectives are essentially identical to Dreamer V2.
\end{frame}

\begin{frame}
    \frametitle{Atari 100k Benchmark: Overview}
    
    \begin{itemize}
        \item \textbf{Benchmark Description:}
        \begin{itemize}
            \item 26 Atari games with only 100k actions (\( \approx \) 2 hours of gameplay)
            \item 500 \( \times \) fewer steps than standard Atari benchmarks (50M steps)
            \item Tests sample efficiency in reinforcement learning
        \end{itemize}
        
        \item \textbf{Compared Methods:}
        \begin{itemize}
            \item \textbf{Without search:} SimPLe, CURL, DrQ, SPR
            \item \textbf{With search:} MuZero, EfficientZero (use planning)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{IRIS Performance Results}
    
    \begin{itemize}
        \item \textbf{Key Achievements:}
        \begin{itemize}
            \item \textbf{Superhuman performance:} 1.046 mean human-normalized score
            \item Outperforms human players in 10/26 games
            \item New state-of-the-art among methods without search
            \item Even surpasses MuZero on this sample-efficient benchmark
            \item (But didn't surpass EfficientZero)
        \end{itemize}
        
        \item \textbf{Game-Specific Performance:}
        \begin{itemize}
            \item Excels in games with predictable dynamics (Pong, Breakout, Boxing)
            \item Struggles with games requiring rare event discovery (Frostbite, Krull)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Insights and Challenges}
    
    \begin{itemize}
        \item \textbf{World Model Importance:}
        \begin{itemize}
            \item Agent learns entirely in imagination
            \item Accurate prediction of objects, rewards, and terminations is crucial
            \item Can generate multiple plausible futures under uncertainty
        \end{itemize}
        
        \item \textbf{Key Challenges:}
        \begin{itemize}
            \item \textbf{"Double Exploration" Problem:}
            \begin{itemize}
                \item Agent must discover rare events/mechanics
                \item World model must learn them before policy can exploit
            \end{itemize}
            \item Complex visual details require higher model capacity
        \end{itemize}
        
        \item \textbf{Conclusion:} World models enable sample-efficient RL when they accurately capture environment dynamics
    \end{itemize}
\end{frame}
\end{document}